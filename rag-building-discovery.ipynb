{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf exploration and preparation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal is to check if we can read the pdf, extract only relevant content, see how we can post-process the extracted text and finally have some informations about the extracted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to `Simple Local RAG Tutorial` :\n",
    "* [GitHub](https://github.com/mrdbourke/simple-local-rag) ;\n",
    "* [YouTube](https://youtu.be/qN_2fnOPY-M?si=APnkpsGY0z_scJ9Z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set files path for pdf document and embeddings\n",
    "PDF_FILENAME = \"source.pdf\"\n",
    "EMBEDDINGS_FILENAME = \"embeddings.csv\"\n",
    "\n",
    "p = Path()\n",
    "\n",
    "pdf_filepath = p.resolve() / \"pdf\" / PDF_FILENAME\n",
    "embeddings_filepath = p.resolve() / \"datasets\" / EMBEDDINGS_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the pdf pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can verify that our source pdf is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pdf file path : '/home/anquetos/gcp-professional-data-engineer-rag/pdf/source.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Check if pdf file is available\n",
    "if pdf_filepath.is_file():\n",
    "    print(f\"Pdf file path : '{pdf_filepath}'.\")\n",
    "else:\n",
    "    print(\"No pdf file found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the number of pages found is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Expected number of pages : \t355\n",
      "* Number of pages found : \t355\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    print(\n",
    "        f\"* Expected number of pages : \\t355\\n* Number of pages found : \\t{len(pdf.pages)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ok, we can try to extract text from a random test page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipelines are sequences of operations that copy, trans-\n",
      "form, load, and analyze data.\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[101]\n",
    "    text = page.extract_text()\n",
    "    print(text[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction works but the text doesn't correspond to the one in the selected page above. First thing to take in account is the fact that the first item in a list is at index 0. So when we write `page = pdf.pages[101]`, in fact it is the page 102 which is extracted.\n",
    "But it is still not ok : the extracted text correpond to page 62 which means page 1 in the pdf is actually the page 41 (index 40). The reason is all the \"About\", \"Introduction\", etc. sections are not numbered the same way in the pdf file.\n",
    "This is something to take in account to extract the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target relevant text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents can have several information which are not relevant to build a RAG :\n",
    "* headers and footers ;\n",
    "* tables ;\n",
    "* hyperlink ;\n",
    "* figures ;\n",
    "* etc..\n",
    "\n",
    "We only want to keep the body of the document but also the code samples even if a part of this last is not always relevant. Since each document is different, there is not a unique method to determine what is relevant or not. The only way to handle this is to take time to inspect the document structure, layout, etc..\n",
    "\n",
    "In my case, it appears that the **font** will be the best way to help me target the body and the code.\n",
    "\n",
    "> Take note that working with fonts means we will extract the text character by character to access its properties thanks to the [`chars` object](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#objects) available for each instance of `pdfplumber.PDF` and `pdfplumber.Page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Header fontname : \tGHSRZR+UniversLTStd\n",
      "* Body fontname : \tGHSRZR+SabonLTStd-Roman\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[43]\n",
    "    header_font = page.chars[3].get(\"fontname\")\n",
    "    body_font = page.chars[103].get(\"fontname\")\n",
    "    print(f\"* Header fontname : \\t{header_font}\\n* Body fontname : \\t{body_font}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample code fontname : \tGHSRZR+SourceCodePro-Regular\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[50]\n",
    "    code_font = page.extract_text_lines(return_chars=True)[8][\"chars\"][0].get(\n",
    "        \"fontname\"\n",
    "    )\n",
    "    print(f\"* Sample code fontname : \\t{code_font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header, body and code have different fonts which is of great help. The last thing to take care of is the fact that the text we want to target can be *italic* or **bold**. So let's make a list of all available fonts in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all fonts in the document\n",
    "fontname_list = []\n",
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        [\n",
    "            fontname_list.append(char.get(\"fontname\"))\n",
    "            for char in page.chars\n",
    "            if char.get(\"fontname\") not in fontname_list\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GHSRZR+SabonLTStd-Roman', 'GHSRZR+SourceCodePro-Regular', 'GHSRZR+SabonLTStd-Bold', 'GHSRZR+SabonLTStd-Italic', 'URTXBU+SourceCodePro-Bold']\n"
     ]
    }
   ],
   "source": [
    "# List only the necessary fonts\n",
    "body_fontname_list = [\n",
    "    fontname\n",
    "    for fontname in fontname_list\n",
    "    if \"Sabon\" in fontname or \"SourceCode\" in fontname\n",
    "]\n",
    "print(body_fontname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step for the font part : we will create a helper function to filter the extracted text by font using the fontname of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font filter helper funtion\n",
    "def filter_text_by_font(chars: list[dict], target_fonts: list[str]) -> str:\n",
    "    \"\"\"Filters extracted text and, more precisely, its letters by their fonts.\n",
    "\n",
    "    Args:\n",
    "        chars (list[dict]): chars object from pdfplumber.\n",
    "        target_fonts (list[str]): list of fontnames for which we want to keep the characters/text.\n",
    "\n",
    "    Returns:\n",
    "        str: filtered text.\n",
    "    \"\"\"\n",
    "    char_text = [char[\"text\"] for char in chars if char.get(\"fontname\") in target_fonts]\n",
    "    text = \"\".join(char_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to have the cleanest text as possible for further steps. We will remove uppercase and unecessary spaces. In addition to that, we will also replace *fifi* string by *fi*. This is a specific error I noticed after the extraction of my document which shows how important it is to inspect each document carefully to identify the best way to process it.\n",
    "Here is a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text formatter function\n",
    "def basic_text_formatter(text: str) -> str:\n",
    "    \"\"\"Applies different operations to format and clean the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: formatted text.\n",
    "    \"\"\"\n",
    "    formatted_text = \" \".join(\n",
    "        text.casefold().replace(\"\\n\", \" \").replace(\"fifi\", \"fi\").split()\n",
    "    )\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \t I'm a Basic   text sample. \n",
      "* After : \ti'm a basic text sample.\n"
     ]
    }
   ],
   "source": [
    "basic_text_sample = \" I'm a Basic   text sample. \"\n",
    "\n",
    "print(\n",
    "    f\"* Before : \\t{basic_text_sample}\\n* After : \\t{basic_text_formatter(basic_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens are used to break words so that the appearance of the page is nicer but it will interfere in the words recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con-\n",
      "necting\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[237]\n",
    "    text = page.extract_text()\n",
    "    hyphen_text_sample = text[1066:1078]\n",
    "    print(hyphen_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(text: str) -> str:\n",
    "    \"\"\"Removes hyphens from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: processed text.\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # Find dashes\n",
    "    line_numbers = []\n",
    "    for line_no, line in enumerate(lines[:-1]):\n",
    "        if line.endswith(\"-\"):\n",
    "            line_numbers.append(line_no)\n",
    "\n",
    "    # Replace\n",
    "    for line_no in line_numbers:\n",
    "        lines = dehyphenate(lines, line_no)\n",
    "\n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "def dehyphenate(lines: list[str], line_no: int) -> list[str]:\n",
    "    \"\"\"Rebuilds lines (words) separated by hyphen.\n",
    "\n",
    "    Args:\n",
    "        lines (list[str]): lines to process.\n",
    "        line_no (int): index of lines to process.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of modified lines.\n",
    "    \"\"\"\n",
    "    next_line = lines[line_no + 1]\n",
    "    word_suffix = next_line.split(\" \")[0]\n",
    "\n",
    "    lines[line_no] = lines[line_no][:-1] + word_suffix\n",
    "    lines[line_no + 1] = lines[line_no + 1][len(word_suffix) :]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \tcon-\n",
      "necting\n",
      "* After : \tconnecting \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"* Before : \\t{hyphen_text_sample}\\n* After : \\t{remove_hyphens(hyphen_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our \"tools\"\" to extract the pdf pages correctly and in a relevant way. To refine a bit more our target will remove the pages we don't want to keep (like introduction, glossary, etc.) and we will skip the blank pages (with no content).\n",
    "\n",
    "Do do this, We will write a final function to process our whole document. Pages will be stored in a list of dictionnaries where we will be able to add information like page number, number of characters, tokens, sentences, etc.. and to explore the pages information by converting it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_pdf(path: Path) -> list[dict]:\n",
    "    \"\"\"Open a pdf file with pdfplumber, extracts and formats relevant pages then append\n",
    "    their content and statistics in a list.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Pathlib path of the document.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Extracted content and informations of pages.\n",
    "    \"\"\"\n",
    "    extracted_pages = []\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            page_number = page_idx - 39\n",
    "            lines = page.extract_text_lines(return_chars=True, keep_blank_chars=True)\n",
    "\n",
    "            kept_lines = []\n",
    "            for line in lines:\n",
    "                kept_lines.append(\n",
    "                    filter_text_by_font(line[\"chars\"], body_fontname_list)\n",
    "                )\n",
    "            text = \"\\n\".join(kept_lines)\n",
    "\n",
    "            text = remove_hyphens(text)\n",
    "            text = basic_text_formatter(text)\n",
    "\n",
    "            if 0 < page_number <= 305 and text:\n",
    "                extracted_pages.append(\n",
    "                    {\n",
    "                        \"page_number\": page_number,\n",
    "                        \"page_chars_count\": len(text),\n",
    "                        \"page_words_count\": len(text.split(\" \")),\n",
    "                        \"page_raw_sentences_count\": len(re.split(r\"[.?!]\", text)),\n",
    "                        \"page_text\": text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return extracted_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_pages = extract_and_process_pdf(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting pages text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we must take in account the fact that we will use the `sentence-transformers` model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) which as *a max tokens capacity of **514***. The token capacity *is very a important concept for a model* since it refers to *the maximum number of tokens it can process* in its context window during a single interaction.\n",
    "\n",
    "In our case, the `all-mpnet-base-v2` model has been trained to ingest and turn into embeddings texts with 514 tokens. Texts over 514 tokens which are encoded by this model will be automatically reduced to 514 tokens in length, potentially losing some information.\n",
    "\n",
    "So what we want to know is how many tokens we have per page. We wil start by a raw tokens counts using the method explained [here](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) or [here](https://python.langchain.com/docs/concepts/tokens/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count\n",
       "mean           2082.93            331.97                     21.32\n",
       "min             121.00             22.00                      1.00\n",
       "max            3705.00            624.00                     55.00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the extracted pages dictionnaries to DataFrame\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[[\"mean\", \"min\", \"max\"]].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know add the `raw_token_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  \n",
       "mean                 520.38  \n",
       "50%                  540.00  \n",
       "75%                  653.00  \n",
       "min                   30.00  \n",
       "max                  926.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate raw tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_raw_tokens_count\"] = len(page[\"page_text\"]) // 4\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can see that the average raw count per page is above the tokens capacity of our model. But is only a raw count so let's try to be more precise.\n",
    "\n",
    "For the next step to come, we will use the *LangChain* framework and its different tools. We will instantiate a `SentenceTransformersTokenTextSplitter` and use the `count_token` method. What is nice is the fact that it is a specialized text splitter for use with `sentence-transformer` models. This means it will behave taking in account the model we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \n",
       "mean                 520.38                  431.07  \n",
       "50%                  540.00                  436.00  \n",
       "75%                  653.00                  544.00  \n",
       "min                   30.00                   29.00  \n",
       "max                  926.00                  785.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Calculate real tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_real_tokens_count\"] = text_splitter.count_tokens(text=page[\"page_text\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. The average tokens count is below the capacity of the model but *we still have 25 % of pages with more than 544 tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chunks/split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to split our text in manageable chunks with the right amount of token. And will to that with using the `split_text` method of `SentenceTransformersTokenTextSplitter`.\n",
    "\n",
    "When splitting the text, we will configure a chunk overlap which define the number of characters which overlap between chunks ensuring that context is preserved. Take in mind that increasing the overlap will increase the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "      <th>page_chunks_max_tokens_count</th>\n",
       "      <th>page_chunks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "      <td>343.94</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \\\n",
       "mean                 520.38                  431.07   \n",
       "50%                  540.00                  436.00   \n",
       "75%                  653.00                  544.00   \n",
       "min                   30.00                   29.00   \n",
       "max                  926.00                  785.00   \n",
       "\n",
       "      page_chunks_max_tokens_count  page_chunks_count  \n",
       "mean                        343.94               1.82  \n",
       "50%                         386.00               2.00  \n",
       "75%                         386.00               2.00  \n",
       "min                          29.00               1.00  \n",
       "max                         389.00               4.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=200, model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Split the text for each page\n",
    "for page in extracted_pages:\n",
    "    page[\"page_chunks\"] = text_splitter.split_text(text=page[\"page_text\"])\n",
    "    page[\"page_chunks_max_tokens_count\"] = max(\n",
    "        text_splitter.count_tokens(text=chunk) for chunk in page[\"page_chunks\"]\n",
    "    )\n",
    "    page[\"page_chunks_count\"] = len(page[\"page_chunks\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice the number of tokens now fits with the model capacity, we are ready to embed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(pages_dict: list[dict]) -> list[dict]:\n",
    "    \"\"\"Calculate embeddings for chunks of text in each page using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        pages_dict (list[dict]): A list of dictionaries where each dictionary represents a page\n",
    "        with its text chunks and metadata.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary contains the source page number,\n",
    "        the text chunk, and its corresponding embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if CUDA is available and set the device\n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    if cuda_available:\n",
    "        device = \"cuda\"\n",
    "        print(f\"CUDA is available : device set to {device.upper()}.\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        print(f\"CUDA is not available : device set to {device.upper()}.\")\n",
    "\n",
    "    # Instantiate the SentenceTransformer model\n",
    "    embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")\n",
    "\n",
    "    # Calculate embeddings for each chunk of text for each page\n",
    "    embeddings = []\n",
    "\n",
    "    for page in extracted_pages:\n",
    "        for chunk in page[\"page_chunks\"]:\n",
    "            embedding = embedding_model.encode(\n",
    "                sentences=chunk, batch_size=32, device=device, normalize_embeddings=True\n",
    "            )\n",
    "            embeddings.append(\n",
    "                {\n",
    "                    \"source_id\": page[\"page_number\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file found : '/home/anquetos/gcp-professional-data-engineer-rag/datasets/embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings file is available and calculate embeddings if not\n",
    "if not embeddings_filepath.is_file():\n",
    "    print(\"Embeddings file not found, calculating embeddings...\")\n",
    "    # Calculate embeddings\n",
    "    embeddings = calculate_embeddings(extracted_pages)\n",
    "    # Save embeddings to a CSV file\n",
    "    pd.DataFrame(embeddings).to_csv(embeddings_filepath, index=False)\n",
    "else:\n",
    "    print(f\"Embeddings file found : '{embeddings_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>data engineers choose how to store data for ma...</td>\n",
       "      <td>[0.035572313, 0.03913909, -0.028386826, -0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>the data lifecycle consists of four stages : i...</td>\n",
       "      <td>[0.039683174, -0.013291235, -0.04148462, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>part of a sales transaction. it also includes ...</td>\n",
       "      <td>[-0.005307973, -0.0408703, -0.017561762, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>streaming data is a set of data that is typica...</td>\n",
       "      <td>[-0.009797745, -0.042565953, -0.023716504, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>if a stream of data needs to be in time order ...</td>\n",
       "      <td>[0.011917694, -0.02348141, -0.024960654, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id                                               text  \\\n",
       "0          2  data engineers choose how to store data for ma...   \n",
       "1          3  the data lifecycle consists of four stages : i...   \n",
       "2          3  part of a sales transaction. it also includes ...   \n",
       "3          4  streaming data is a set of data that is typica...   \n",
       "4          4  if a stream of data needs to be in time order ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.035572313, 0.03913909, -0.028386826, -0.036...  \n",
       "1  [0.039683174, -0.013291235, -0.04148462, -0.01...  \n",
       "2  [-0.005307973, -0.0408703, -0.017561762, 0.028...  \n",
       "3  [-0.009797745, -0.042565953, -0.023716504, -0....  \n",
       "4  [0.011917694, -0.02348141, -0.024960654, -0.02...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load embeddings from the CSV file\n",
    "df_embeddings = pd.read_csv(embeddings_filepath)\n",
    "# Convert the string representation of the embeddings back to numpy 'float32' arrays \n",
    "# (original output format from SentenceTransformer encode method)\n",
    "df_embeddings[\"embedding\"] = df_embeddings[\"embedding\"].apply(\n",
    "    lambda x: np.array(x.strip(\"[]\").split(), dtype=\"float32\")\n",
    ")\n",
    "\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R.A.G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Tensor shape : torch.Size([519, 768])\n",
      "* Tensor type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Convert the embeddings to PyTorch tensors\n",
    "vectors_tensor = torch.from_numpy(np.stack(df_embeddings[\"embedding\"].values))\n",
    "\n",
    "# Check the shape and type of the tensor\n",
    "print(f\"* Tensor shape : {vectors_tensor.shape}\")\n",
    "print(f\"* Tensor type : {vectors_tensor.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
