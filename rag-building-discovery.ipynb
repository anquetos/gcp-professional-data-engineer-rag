{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf exploration and preparation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal is to check if we can read the pdf, extract only relevant content, see how we can post-process the extracted text and finally have some informations about the extracted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to `Simple Local RAG Tutorial` :\n",
    "* [GitHub](https://github.com/mrdbourke/simple-local-rag) ;\n",
    "* [YouTube](https://youtu.be/qN_2fnOPY-M?si=APnkpsGY0z_scJ9Z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from utils.timing_functions import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set files path for pdf document and embeddings\n",
    "PDF_FILENAME = \"source.pdf\"\n",
    "EMBEDDINGS_FILENAME = \"embeddings.csv\"\n",
    "\n",
    "p = Path()\n",
    "\n",
    "pdf_filepath = p.resolve() / \"pdf\" / PDF_FILENAME\n",
    "embeddings_filepath = p.resolve() / \"datasets\" / EMBEDDINGS_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the pdf pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can verify that our source pdf is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pdf file path : '/home/anquetos/gcp-professional-data-engineer-rag/pdf/source.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Check if pdf file is available\n",
    "if pdf_filepath.is_file():\n",
    "    print(f\"Pdf file path : '{pdf_filepath}'.\")\n",
    "else:\n",
    "    print(\"No pdf file found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the number of pages found is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Expected number of pages : \t355\n",
      "* Number of pages found : \t355\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    print(\n",
    "        f\"* Expected number of pages : \\t355\\n* Number of pages found : \\t{len(pdf.pages)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ok, we can try to extract text from a random test page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipelines are sequences of operations that copy, trans-\n",
      "form, load, and analyze data.\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[101]\n",
    "    text = page.extract_text()\n",
    "    print(text[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction works but the text doesn't correspond to the one in the selected page above. First thing to take in account is the fact that the first item in a list is at index 0. So when we write `page = pdf.pages[101]`, in fact it is the page 102 which is extracted.\n",
    "But it is still not ok : the extracted text correpond to page 62 which means page 1 in the pdf is actually the page 41 (index 40). The reason is all the \"About\", \"Introduction\", etc. sections are not numbered the same way in the pdf file.\n",
    "This is something to take in account to extract the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target relevant text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents can have several information which are not relevant to build a RAG :\n",
    "* headers and footers ;\n",
    "* tables ;\n",
    "* hyperlink ;\n",
    "* figures ;\n",
    "* etc..\n",
    "\n",
    "We only want to keep the body of the document but also the code samples even if a part of this last is not always relevant. Since each document is different, there is not a unique method to determine what is relevant or not. The only way to handle this is to take time to inspect the document structure, layout, etc..\n",
    "\n",
    "In my case, it appears that the **font** will be the best way to help me target the body and the code.\n",
    "\n",
    "> Take note that working with fonts means we will extract the text character by character to access its properties thanks to the [`chars` object](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#objects) available for each instance of `pdfplumber.PDF` and `pdfplumber.Page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Header fontname : \tGHSRZR+UniversLTStd\n",
      "* Body fontname : \tGHSRZR+SabonLTStd-Roman\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[43]\n",
    "    header_font = page.chars[3].get(\"fontname\")\n",
    "    body_font = page.chars[103].get(\"fontname\")\n",
    "    print(f\"* Header fontname : \\t{header_font}\\n* Body fontname : \\t{body_font}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample code fontname : \tGHSRZR+SourceCodePro-Regular\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[50]\n",
    "    code_font = page.extract_text_lines(return_chars=True)[8][\"chars\"][0].get(\n",
    "        \"fontname\"\n",
    "    )\n",
    "    print(f\"* Sample code fontname : \\t{code_font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header, body and code have different fonts which is of great help. The last thing to take care of is the fact that the text we want to target can be *italic* or **bold**. So let's make a list of all available fonts in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all fonts in the document\n",
    "fontname_list = []\n",
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        [\n",
    "            fontname_list.append(char.get(\"fontname\"))\n",
    "            for char in page.chars\n",
    "            if char.get(\"fontname\") not in fontname_list\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GHSRZR+SabonLTStd-Roman', 'GHSRZR+SourceCodePro-Regular', 'GHSRZR+SabonLTStd-Bold', 'GHSRZR+SabonLTStd-Italic', 'URTXBU+SourceCodePro-Bold']\n"
     ]
    }
   ],
   "source": [
    "# List only the necessary fonts\n",
    "body_fontname_list = [\n",
    "    fontname\n",
    "    for fontname in fontname_list\n",
    "    if \"Sabon\" in fontname or \"SourceCode\" in fontname\n",
    "]\n",
    "print(body_fontname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step for the font part : we will create a helper function to filter the extracted text by font using the fontname of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font filter helper funtion\n",
    "def filter_text_by_font(chars: list[dict], target_fonts: list[str]) -> str:\n",
    "    \"\"\"Filters extracted text and, more precisely, its letters by their fonts.\n",
    "\n",
    "    Args:\n",
    "        chars (list[dict]): chars object from pdfplumber.\n",
    "        target_fonts (list[str]): list of fontnames for which we want to keep the characters/text.\n",
    "\n",
    "    Returns:\n",
    "        str: filtered text.\n",
    "    \"\"\"\n",
    "    char_text = [char[\"text\"] for char in chars if char.get(\"fontname\") in target_fonts]\n",
    "    text = \"\".join(char_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to have the cleanest text as possible for further steps. We will remove uppercase and unecessary spaces. In addition to that, we will also replace *fifi* string by *fi*. This is a specific error I noticed after the extraction of my document which shows how important it is to inspect each document carefully to identify the best way to process it.\n",
    "Here is a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text formatter function\n",
    "def basic_text_formatter(text: str) -> str:\n",
    "    \"\"\"Applies different operations to format and clean the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: formatted text.\n",
    "    \"\"\"\n",
    "    formatted_text = \" \".join(\n",
    "        text.casefold().replace(\"\\n\", \" \").replace(\"fifi\", \"fi\").split()\n",
    "    )\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \t I'm a Basic   text sample. \n",
      "* After : \ti'm a basic text sample.\n"
     ]
    }
   ],
   "source": [
    "basic_text_sample = \" I'm a Basic   text sample. \"\n",
    "\n",
    "print(\n",
    "    f\"* Before : \\t{basic_text_sample}\\n* After : \\t{basic_text_formatter(basic_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens are used to break words so that the appearance of the page is nicer but it will interfere in the words recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con-\n",
      "necting\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[237]\n",
    "    text = page.extract_text()\n",
    "    hyphen_text_sample = text[1066:1078]\n",
    "    print(hyphen_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(text: str) -> str:\n",
    "    \"\"\"Removes hyphens from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: processed text.\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # Find dashes\n",
    "    line_numbers = []\n",
    "    for line_no, line in enumerate(lines[:-1]):\n",
    "        if line.endswith(\"-\"):\n",
    "            line_numbers.append(line_no)\n",
    "\n",
    "    # Replace\n",
    "    for line_no in line_numbers:\n",
    "        lines = dehyphenate(lines, line_no)\n",
    "\n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "def dehyphenate(lines: list[str], line_no: int) -> list[str]:\n",
    "    \"\"\"Rebuilds lines (words) separated by hyphen.\n",
    "\n",
    "    Args:\n",
    "        lines (list[str]): lines to process.\n",
    "        line_no (int): index of lines to process.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of modified lines.\n",
    "    \"\"\"\n",
    "    next_line = lines[line_no + 1]\n",
    "    word_suffix = next_line.split(\" \")[0]\n",
    "\n",
    "    lines[line_no] = lines[line_no][:-1] + word_suffix\n",
    "    lines[line_no + 1] = lines[line_no + 1][len(word_suffix) :]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \tcon-\n",
      "necting\n",
      "* After : \tconnecting \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"* Before : \\t{hyphen_text_sample}\\n* After : \\t{remove_hyphens(hyphen_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our \"tools\"\" to extract the pdf pages correctly and in a relevant way. To refine a bit more our target will remove the pages we don't want to keep (like introduction, glossary, etc.) and we will skip the blank pages (with no content).\n",
    "\n",
    "Do do this, We will write a final function to process our whole document. Pages will be stored in a list of dictionnaries where we will be able to add information like page number, number of characters, tokens, sentences, etc.. and to explore the pages information by converting it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_pdf(path: Path) -> list[dict]:\n",
    "    \"\"\"Open a pdf file with pdfplumber, extracts and formats relevant pages then append\n",
    "    their content and statistics in a list.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Pathlib path of the document.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Extracted content and informations of pages.\n",
    "    \"\"\"\n",
    "    extracted_pages = []\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            page_number = page_idx - 39\n",
    "            lines = page.extract_text_lines(return_chars=True, keep_blank_chars=True)\n",
    "\n",
    "            kept_lines = []\n",
    "            for line in lines:\n",
    "                kept_lines.append(\n",
    "                    filter_text_by_font(line[\"chars\"], body_fontname_list)\n",
    "                )\n",
    "            text = \"\\n\".join(kept_lines)\n",
    "\n",
    "            text = remove_hyphens(text)\n",
    "            text = basic_text_formatter(text)\n",
    "\n",
    "            if 0 < page_number <= 305 and text:\n",
    "                extracted_pages.append(\n",
    "                    {\n",
    "                        \"page_number\": page_number,\n",
    "                        \"page_chars_count\": len(text),\n",
    "                        \"page_words_count\": len(text.split(\" \")),\n",
    "                        \"page_raw_sentences_count\": len(re.split(r\"[.?!]\", text)),\n",
    "                        \"page_text\": text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return extracted_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process pdf\n",
    "extracted_pages = extract_and_process_pdf(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting pages text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we must take in account the fact that we will use the `sentence-transformers` model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) which as *a max tokens capacity of **514***. The token capacity *is very a important concept for a model* since it refers to *the maximum number of tokens it can process* in its context window during a single interaction.\n",
    "\n",
    "In our case, the `all-mpnet-base-v2` model has been trained to ingest and turn into embeddings texts with 514 tokens. Texts over 514 tokens which are encoded by this model will be automatically reduced to 514 tokens in length, potentially losing some information.\n",
    "\n",
    "So what we want to know is how many tokens we have per page. We wil start by a raw tokens counts using the method explained [here](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) or [here](https://python.langchain.com/docs/concepts/tokens/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count\n",
       "mean           2082.93            331.97                     21.32\n",
       "min             121.00             22.00                      1.00\n",
       "max            3705.00            624.00                     55.00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the extracted pages dictionnaries to DataFrame\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[[\"mean\", \"min\", \"max\"]].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know add the `raw_token_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  \n",
       "mean                 520.38  \n",
       "50%                  540.00  \n",
       "75%                  653.00  \n",
       "min                   30.00  \n",
       "max                  926.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate raw tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_raw_tokens_count\"] = len(page[\"page_text\"]) // 4\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can see that the average raw count per page is above the tokens capacity of our model. But is only a raw count so let's try to be more precise.\n",
    "\n",
    "For the next step to come, we will use the *LangChain* framework and its different tools. We will instantiate a `SentenceTransformersTokenTextSplitter` and use the `count_token` method. What is nice is the fact that it is a specialized text splitter for use with `sentence-transformer` models. This means it will behave taking in account the model we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \n",
       "mean                 520.38                  431.07  \n",
       "50%                  540.00                  436.00  \n",
       "75%                  653.00                  544.00  \n",
       "min                   30.00                   29.00  \n",
       "max                  926.00                  785.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Calculate real tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_real_tokens_count\"] = text_splitter.count_tokens(text=page[\"page_text\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. The average tokens count is below the capacity of the model but *we still have 25 % of pages with more than 544 tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chunks/split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to split our text in manageable chunks with the right amount of token. And will to that with using the `split_text` method of `SentenceTransformersTokenTextSplitter`.\n",
    "\n",
    "When splitting the text, we will configure a chunk overlap which define the number of characters which overlap between chunks ensuring that context is preserved. Take in mind that increasing the overlap will increase the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "      <th>page_chunks_max_tokens_count</th>\n",
       "      <th>page_chunks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "      <td>344.08</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \\\n",
       "mean                 520.38                  431.07   \n",
       "50%                  540.00                  436.00   \n",
       "75%                  653.00                  544.00   \n",
       "min                   30.00                   29.00   \n",
       "max                  926.00                  785.00   \n",
       "\n",
       "      page_chunks_max_tokens_count  page_chunks_count  \n",
       "mean                        344.08               2.38  \n",
       "50%                         386.00               2.00  \n",
       "75%                         386.00               3.00  \n",
       "min                          29.00               1.00  \n",
       "max                         389.00               6.00  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=200, model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Split the text for each page\n",
    "for page in extracted_pages:\n",
    "    page[\"page_chunks\"] = text_splitter.split_text(text=page[\"page_text\"])\n",
    "    page[\"page_chunks_max_tokens_count\"] = max(\n",
    "        text_splitter.count_tokens(text=chunk) for chunk in page[\"page_chunks\"]\n",
    "    )\n",
    "    page[\"page_chunks_count\"] = len(page[\"page_chunks\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice the number of tokens now fits with the model capacity, we are ready to embed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available : device set to CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    device = \"cuda\"\n",
    "    print(f\"CUDA is available : device set to {device.upper()}.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"CUDA is not available : device set to {device.upper()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(pages_dict: list[dict]) -> list[dict]:\n",
    "    \"\"\"Calculate embeddings for chunks of text in each page using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        pages_dict (list[dict]): A list of dictionaries where each dictionary represents a page\n",
    "        with its text chunks and metadata.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary contains the source page number,\n",
    "        the text chunk, and its corresponding embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate embeddings for each chunk of text for each page\n",
    "    embeddings = []\n",
    "\n",
    "    for page in extracted_pages:\n",
    "        for chunk in page[\"page_chunks\"]:\n",
    "            embedding = embedding_model.encode(\n",
    "                sentences=chunk, batch_size=32, device=device, normalize_embeddings=True\n",
    "            )\n",
    "            embeddings.append(\n",
    "                {\n",
    "                    \"source_id\": page[\"page_number\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file found : '/home/anquetos/gcp-professional-data-engineer-rag/datasets/embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings file is available and calculate embeddings if not\n",
    "if not embeddings_filepath.is_file():\n",
    "    print(\"Embeddings file not found, calculating embeddings...\")\n",
    "    # Calculate embeddings\n",
    "    embeddings = calculate_embeddings(extracted_pages)\n",
    "    # Save embeddings to a CSV file\n",
    "    pd.DataFrame(embeddings).to_csv(embeddings_filepath, index=False)\n",
    "else:\n",
    "    print(f\"Embeddings file found : '{embeddings_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>data engineers choose how to store data for ma...</td>\n",
       "      <td>[0.035572313, 0.03913909, -0.028386826, -0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>the data lifecycle consists of four stages : i...</td>\n",
       "      <td>[0.039683174, -0.013291235, -0.04148462, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>part of a sales transaction. it also includes ...</td>\n",
       "      <td>[-0.005307973, -0.0408703, -0.017561762, 0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>streaming data is a set of data that is typica...</td>\n",
       "      <td>[-0.009797745, -0.042565953, -0.023716504, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>if a stream of data needs to be in time order ...</td>\n",
       "      <td>[0.011917694, -0.02348141, -0.024960654, -0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id                                               text  \\\n",
       "0          2  data engineers choose how to store data for ma...   \n",
       "1          3  the data lifecycle consists of four stages : i...   \n",
       "2          3  part of a sales transaction. it also includes ...   \n",
       "3          4  streaming data is a set of data that is typica...   \n",
       "4          4  if a stream of data needs to be in time order ...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.035572313, 0.03913909, -0.028386826, -0.036...  \n",
       "1  [0.039683174, -0.013291235, -0.04148462, -0.01...  \n",
       "2  [-0.005307973, -0.0408703, -0.017561762, 0.028...  \n",
       "3  [-0.009797745, -0.042565953, -0.023716504, -0....  \n",
       "4  [0.011917694, -0.02348141, -0.024960654, -0.02...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load embeddings from the CSV file\n",
    "df_embeddings = pd.read_csv(embeddings_filepath)\n",
    "# Convert the string representation of the embeddings back to numpy 'float32' arrays\n",
    "# (original output format from SentenceTransformer encode method)\n",
    "df_embeddings[\"embedding\"] = df_embeddings[\"embedding\"].apply(\n",
    "    lambda x: np.array(x.strip(\"[]\").split(), dtype=\"float32\")\n",
    ")\n",
    "\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R.A.G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Tensor shape : torch.Size([519, 768])\n",
      "* Tensor type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Convert the embeddings to PyTorch tensors\n",
    "vectors_tensor = torch.from_numpy(np.stack(df_embeddings[\"embedding\"].values))\n",
    "\n",
    "# Check the shape and type of the tensor\n",
    "print(f\"* Tensor shape : {vectors_tensor.shape}\")\n",
    "print(f\"* Tensor type : {vectors_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def search_top_k_vectors(query: str, k: int = 5) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Search for the top k most similar vectors to a query in the embeddings space.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "        k (int, optional): The number of most similar vectors to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: The top k vectors and their corresponding scores.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = embedding_model.encode(\n",
    "        sentences=query, batch_size=32, device=device, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Calculate the dot product similarity between the query embedding and all the embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=vectors_tensor)\n",
    "\n",
    "    # Get the top k most similar vectors\n",
    "    top_k_vectors = torch.topk(dot_scores[0], k=k)\n",
    "\n",
    "    return top_k_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search_from_embeddings(query: str, embeddings: pd.DataFrame, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Perform a semantic search using the provided query and embeddings, and return the top k results.\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        embeddings (pd.DataFrame): A DataFrame containing the embeddings and associated metadata.\n",
    "        k (int, optional): The number of top results to return. Defaults to 5.\n",
    "    Returns:\n",
    "        str: A formatted string containing the query, scores, source IDs, and corresponding texts of the top k results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the top k most similar vectors\n",
    "    top_k = search_top_k_vectors(query=query, k=k)\n",
    "\n",
    "    # Set the datraframe to use\n",
    "    df = embeddings\n",
    "\n",
    "    # Initialize the results text\n",
    "    results_text = \"\"\n",
    "\n",
    "    # Loop through the top k results and format the output\n",
    "    for score, idx in zip(top_k[0], top_k[1]):\n",
    "        source_id = df.loc[idx.item()][\"source_id\"]\n",
    "        query_text = f\"Query : {query}\\n\"\n",
    "        score_text = f\"Score : {score.item():.4f}\\n\"\n",
    "        source_id_text = f\"Id : {source_id}\\n\"\n",
    "        main_text = (\n",
    "            f\"\"\"{(\" \".join(df.loc[df[\"source_id\"] == source_id, \"text\"].values))}\"\"\"\n",
    "        )\n",
    "        results_text += (\n",
    "            query_text\n",
    "            + score_text\n",
    "            + source_id_text\n",
    "            + textwrap.fill(main_text, width=100)\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    return results_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : 'search_top_k_vectors' function executed in 0.06724 seconds.\n",
      "Query : extract transform load\n",
      "score : 0.3544\n",
      "id : 67\n",
      "extract, transformation, and load ( etl ) pipelines begin with extracting data from one or more data\n",
      "sources. when multiple data sources are used, the extraction processes need to be coordinated. this\n",
      "is because extractions are often time based, so it is important that the extracted data cover the\n",
      "same time period. for example, an extraction process may run once an hour and extract data inserted\n",
      "or modified in the previous hour. consider an inventory data warehouse that extracts data once an\n",
      "hour from a database that tracks the number of products in each of the company ’ s storage\n",
      "facilities. products are coded using stock keeping unit ( sku ) codes. a product database maintains\n",
      "the details on each product, such as description, suppliers, and unit costs. a data warehouse would\n",
      "need to extract data from the inventory database for the level of inventory information and from the\n",
      "products database for description information. if a new product is added to the product database and\n",
      "stocked in the warehouse, the data warehouse would need up - to - date data from both source\n",
      "databases ; otherwise, there could be inventory data with no corresponding description data about\n",
      "the product. in an etl pipeline, data is transformed in the pipeline before being stored in a\n",
      "database. in the past, data warehouse developers may have used custom scripts to transform the data\n",
      "or a specialized etl tool that allowed developers to specify transformation steps using a graphical\n",
      "user interface ( gui ). this can work well in cases where the transformation code is already\n",
      "captured in scripts or when data analysts with limited programming experience are building\n",
      "transformations. it does, however, sometimes require developers to learn a tool in addition to sql\n",
      "for manipulating data. in gcp, transformations can be done using cloud dataproc or cloud dataflflow.\n",
      "with cloud dataproc, transformations can be written in a spark - or hadoop - supported language.\n",
      "spark uses an in - memory distributed data model for data manipulation and analysis. spark programs\n",
      "database and stocked in the warehouse, the data warehouse would need up - to - date data from both\n",
      "source databases ; otherwise, there could be inventory data with no corresponding description data\n",
      "about the product. in an etl pipeline, data is transformed in the pipeline before being stored in a\n",
      "database. in the past, data warehouse developers may have used custom scripts to transform the data\n",
      "or a specialized etl tool that allowed developers to specify transformation steps using a graphical\n",
      "user interface ( gui ). this can work well in cases where the transformation code is already\n",
      "captured in scripts or when data analysts with limited programming experience are building\n",
      "transformations. it does, however, sometimes require developers to learn a tool in addition to sql\n",
      "for manipulating data. in gcp, transformations can be done using cloud dataproc or cloud dataflflow.\n",
      "with cloud dataproc, transformations can be written in a spark - or hadoop - supported language.\n",
      "spark uses an in - memory distributed data model for data manipulation and analysis. spark programs\n",
      "can be written in java, scala, python, r, and sql. when using cloud dataproc, your transformations\n",
      "are written according to hadoop ’ s map reduce model or spark ’ s distributed tabular data\n",
      "structure. in addition to supporting java for transformations, hadoop provides pig, a high - level\n",
      "language for data manipulation. pig programs compile into map reduce programs that run on hadoop.\n",
      "when using cloud dataflflow, you write transformations using the apache beam model, which provides a\n",
      "unified batch and stream processing model. apache beam is modeled as a pipeline and has explicit\n",
      "support for pipeline constructs, including the following : pipelines : an encapsulation of the end -\n",
      "to - end data processing task that executes the data operations pcollection : a distributed dataset\n",
      "ptransform : an operation on data, such as grouping by, flflattening, and partitioning of uses an in\n",
      "- memory distributed data model for data manipulation and analysis. spark programs can be written in\n",
      "java, scala, python, r, and sql. when using cloud dataproc, your transformations are written\n",
      "according to hadoop ’ s map reduce model or spark ’ s distributed tabular data structure. in\n",
      "addition to supporting java for transformations, hadoop provides pig, a high - level language for\n",
      "data manipulation. pig programs compile into map reduce programs that run on hadoop. when using\n",
      "cloud dataflflow, you write transformations using the apache beam model, which provides a unified\n",
      "batch and stream processing model. apache beam is modeled as a pipeline and has explicit support for\n",
      "pipeline constructs, including the following : pipelines : an encapsulation of the end - to - end\n",
      "data processing task that executes the data operations pcollection : a distributed dataset\n",
      "ptransform : an operation on data, such as grouping by, flflattening, and partitioning of data\n",
      "apache beam programs are written in java and python. for writing data to a database, cloud\n",
      "dataflflow uses connectors, including bigtable, cloud spanner, and bigquery.\n",
      "\n",
      "query : extract transform load\n",
      "score : 0.3114\n",
      "id : 84\n",
      "know the common patterns in data warehousing pipelines. extract, transformation, and load ( etl )\n",
      "pipelines begin with extracting data from one or more data sources. when multiple data sources are\n",
      "used, the extraction processes need to be coordinated. this is because extractions are often time\n",
      "based, so it is important that extracts from different sources cover the same time period. extract,\n",
      "load, and transformation ( elt ) processes are slightly different from etl processes. in an elt\n",
      "process, data is loaded into a database before transforming the data. extraction and load procedures\n",
      "do not transform data. this kind of process is appropriate when data does not require changes from\n",
      "the source format. in a change data capture approach, each change is a source system that is\n",
      "captured and recorded in a data store. this is helpful in cases where it is important to know all\n",
      "changes over time and not just the state of the database at the time of data extraction. understand\n",
      "the unique processing characteristics of stream processing. this includes the difference between\n",
      "event time and processing time, sliding and tumbling windows, latearriving data and watermarks, and\n",
      "missing data. event time is the time that something occurred at the place where the data is\n",
      "generated. processing time is the time that data arrives at the endpoint where data is ingested.\n",
      "sliding windows are used when you want to show how an aggregate, such as the average of the last\n",
      "three values, change over time, and you want to update that stream of averages each time a new value\n",
      "arrives in the stream. tumbling windows are used when you want to aggregate data over a fixed period\n",
      "of time — for example, for the last one minute. know the components of a typical machine learning\n",
      "pipeline. this includes data ingestion, data preprocessing, feature engineering, model training and\n",
      "evaluation, and deployment. data ingestion uses the same tools and services as data wareho at the\n",
      "time of data extraction. understand the unique processing characteristics of stream processing. this\n",
      "includes the difference between event time and processing time, sliding and tumbling windows,\n",
      "latearriving data and watermarks, and missing data. event time is the time that something occurred\n",
      "at the place where the data is generated. processing time is the time that data arrives at the\n",
      "endpoint where data is ingested. sliding windows are used when you want to show how an aggregate,\n",
      "such as the average of the last three values, change over time, and you want to update that stream\n",
      "of averages each time a new value arrives in the stream. tumbling windows are used when you want to\n",
      "aggregate data over a fixed period of time — for example, for the last one minute. know the\n",
      "components of a typical machine learning pipeline. this includes data ingestion, data preprocessing,\n",
      "feature engineering, model training and evaluation, and deployment. data ingestion uses the same\n",
      "tools and services as data warehousing and streaming data pipelines. cloud storage is used for batch\n",
      "storage of datasets, whereas cloud pub / sub can be used for the ingestion of streaming data.\n",
      "feature engineering is a machine learning practice in which new attributes are introduced into a\n",
      "dataset. the new attributes are derived from one or more existing attributes. know that cloud pub /\n",
      "sub is a managed message queue service. cloud pub / sub is a real - time messaging service that\n",
      "supports both push and pull subscription models. it is a managed service, and it requires no\n",
      "provisioning of servers or clusters. cloud pub / sub will automatically scale as needed. messaging\n",
      "queues are used in distributed systems to decouple services in a pipeline. this allows one service\n",
      "to produce more output than the consuming service can process without adversely affecting the\n",
      "consuming service. this is especially helpful when one process is subject to spikes. know that cloud\n",
      "dataflow is a deployment. data ingestion uses the same tools and services as data warehousing and\n",
      "streaming data pipelines. cloud storage is used for batch storage of datasets, whereas cloud pub /\n",
      "sub can be used for the ingestion of streaming data. feature engineering is a machine learning\n",
      "practice in which new attributes are introduced into a dataset. the new attributes are derived from\n",
      "one or more existing attributes. know that cloud pub / sub is a managed message queue service. cloud\n",
      "pub / sub is a real - time messaging service that supports both push and pull subscription models.\n",
      "it is a managed service, and it requires no provisioning of servers or clusters. cloud pub / sub\n",
      "will automatically scale as needed. messaging queues are used in distributed systems to decouple\n",
      "services in a pipeline. this allows one service to produce more output than the consuming service\n",
      "can process without adversely affecting the consuming service. this is especially helpful when one\n",
      "process is subject to spikes. know that cloud dataflow is a managed stream and batch processing\n",
      "service. cloud dataflflow is a core component for running pipelines that collect, transform, and\n",
      "output data. in the past, developers would typically create a stream processing pipeline ( hot path\n",
      ") and a separate batch processing pipeline ( cold path ). cloud dataflflow is based on apache\n",
      "\n",
      "query : extract transform load\n",
      "score : 0.2817\n",
      "id : 64\n",
      "transformation is the process of mapping data from the structure used in the source system to the\n",
      "structure used in the storage and analysis stages of the data pipeline. there are many kinds of\n",
      "transformations, including the following : converting data types, such as converting a text\n",
      "representation of a date to a datetime data type substituting missing data with default or imputed\n",
      "values aggregating data ; for example, averaging all cpu utilization metrics for an instance over\n",
      "the course of one minute filtering records that violate business logic rules, such as an audit log\n",
      "transaction with a date in the future augmenting data by joining records from distinct sources, such\n",
      "as joining data from an employee table with data from a sales table that includes the employee\n",
      "identifier of the person who made the sale dropping columns or attributes from a dataset when they\n",
      "will not be needed adding columns or attributes derived from input data ; for example, the average\n",
      "of the previous three reported sales prices of a stock might be added to a row of data about the\n",
      "latest price for that stock in gcp, cloud dataflflow and cloud dataproc are often used for\n",
      "transformation stages of both batch and streaming data. cloud dataprep is used for interactive\n",
      "review and preparation of data for analysis. cloud datafusion can be used for the same purpose, and\n",
      "it is more popular with enterprise customers ( see figure 3. 4 ). after data is ingested and\n",
      "transformed, it is often stored. chapter 2, “ building and operationalizing storage systems, ”\n",
      "describes gcp storage systems in detail, but key points related to data pipelines will be reviewed\n",
      "here as well.\n",
      "\n",
      "query : extract transform load\n",
      "score : 0.2704\n",
      "id : 68\n",
      "cloud dataproc is a good choice for implementing etl processes if you are migrating existing hadoop\n",
      "or spark programs. cloud dataflflow is the recommended tool for developing new etl processes. cloud\n",
      "dataflflow is serverless, so there is no cluster to manage and the processing model is based on data\n",
      "pipelines. cloud dataproc ’ s hadoop and spark platforms are designed on big data analytics\n",
      "processing, so they can be used for transformations, but cloud dataflflow model is based on data\n",
      "pipelines. extract, load, and transformation ( elt ) processes are slightly different from etl\n",
      "processes. in an elt process, data is loaded into a database before transforming the data. this\n",
      "process has some advantages over etl. when data is loaded before transformation, the database will\n",
      "contain the original data as extracted. this enables data warehouse developers to query the data\n",
      "using sql, which can be useful for performing basic data quality checks and collecting statistics on\n",
      "characteristics such as the number of rows with missing data. a second advantage is that developers\n",
      "can use sql for transformation operations. this is especially helpful if developers are well versed\n",
      "in sql but do not have programming experience. developers would also be able to use sql tools in\n",
      "addition to writing sql from scratch. extraction and load procedures do not transform data. this\n",
      "type of process is appropriate when data does not require changes from the source format. log data,\n",
      "for example, may be extracted and loaded without transformation. dimensional data extracted from a\n",
      "data warehouse for loading into a smaller data mart also may not need transformation. in a change\n",
      "data capture approach, each change in a source system is captured and recorded in a data store. this\n",
      "is helpful in cases where it is important to know all changes over time and not just the state of\n",
      "the database at the time of data extraction. for example, an inventory database tracking the number\n",
      "of units of products available in a warehouse may have performing basic data quality checks and\n",
      "collecting statistics on characteristics such as the number of rows with missing data. a second\n",
      "advantage is that developers can use sql for transformation operations. this is especially helpful\n",
      "if developers are well versed in sql but do not have programming experience. developers would also\n",
      "be able to use sql tools in addition to writing sql from scratch. extraction and load procedures do\n",
      "not transform data. this type of process is appropriate when data does not require changes from the\n",
      "source format. log data, for example, may be extracted and loaded without transformation.\n",
      "dimensional data extracted from a data warehouse for loading into a smaller data mart also may not\n",
      "need transformation. in a change data capture approach, each change in a source system is captured\n",
      "and recorded in a data store. this is helpful in cases where it is important to know all changes\n",
      "over time and not just the state of the database at the time of data extraction. for example, an\n",
      "inventory database tracking the number of units of products available in a warehouse may have the\n",
      "following changes : product a ’ s inventory is set to 500 in warehouse 1. product b ’ s inventory is\n",
      "set to 300 in warehouse 1. product a ’ s inventory is reduced by 250 in warehouse 1. product a ’ s\n",
      "inventory is reduced by 100 in warehouse 1. after these changes, product a ’ s inventory level is\n",
      "150 and product b ’ s is 300. if you need to know only the final inventory level, then an etl or elt\n",
      "process is sufficient ; however, if you need to know all the changes in inventory levels of\n",
      "products, then a change data capture approach is better.\n",
      "\n",
      "query : extract transform load\n",
      "score : 0.2410\n",
      "id : 66\n",
      "the basic four - stage data pipeline pattern can take on more specific characteristics for different\n",
      "kinds of pipelines. the structure and function of data pipelines will vary according to the use case\n",
      "to which they are applied, but three common types of pipelines are as follows : data warehousing\n",
      "pipelines stream processing pipelines machine learning pipeline let ’ s take a look at each in more\n",
      "detail. data warehouses are databases for storing data from multiple data sources, typically\n",
      "organized in a dimensional data model. dimensional data models are denormalized ; that is, they do\n",
      "not adhere to the rules of normalization used in transaction processing systems. this is done\n",
      "intentionally because the purpose of a data warehouse is to answer analytic queries efficiently, and\n",
      "highly normalized data models can require complex joins and significant amounts of i / o operations.\n",
      "denormalized dimensional models keep related data together in a minimal number of tables so that few\n",
      "joins are required. collecting and restructuring data from online transaction processing systems is\n",
      "often a multistep process. some common patterns in data warehousing pipelines are as follows :\n",
      "extraction, transformation, and load ( etl ) extraction, load, and transformation ( elt ) extraction\n",
      "and load change data capture these are often batch processing pipelines, but they can have some\n",
      "characteristics of streaming pipelines, especially in the case of change data capture.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search using the indicated query\n",
    "result = semantic_search_from_embeddings(\n",
    "    \"extract transform load\", df_embeddings\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
