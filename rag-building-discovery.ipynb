{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf exploration and preparation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal is to check if we can read the pdf, extract only relevant content, see how we can post-process the extracted text and finally have some informations about the extracted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to `Simple Local RAG Tutorial` :\n",
    "* [GitHub](https://github.com/mrdbourke/simple-local-rag) ;\n",
    "* [YouTube](https://youtu.be/qN_2fnOPY-M?si=APnkpsGY0z_scJ9Z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from utils.timing_functions import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set files path for pdf document and embeddings\n",
    "PDF_FILENAME = \"source.pdf\"\n",
    "EMBEDDINGS_FILENAME = \"embeddings.csv\"\n",
    "\n",
    "p = Path()\n",
    "\n",
    "pdf_filepath = p.resolve() / \"pdf\" / PDF_FILENAME\n",
    "embeddings_filepath = p.resolve() / \"datasets\" / EMBEDDINGS_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the pdf pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can verify that our source pdf is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pdf file path : '/home/anquetos/gcp-professional-data-engineer-rag/pdf/source.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Check if pdf file is available\n",
    "if pdf_filepath.is_file():\n",
    "    print(f\"Pdf file path : '{pdf_filepath}'.\")\n",
    "else:\n",
    "    print(\"No pdf file found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the number of pages found is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Expected number of pages : \t355\n",
      "* Number of pages found : \t355\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    print(\n",
    "        f\"* Expected number of pages : \\t355\\n* Number of pages found : \\t{len(pdf.pages)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ok, we can try to extract text from a random test page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipelines are sequences of operations that copy, trans-\n",
      "form, load, and analyze data.\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[101]\n",
    "    text = page.extract_text()\n",
    "    print(text[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction works but the text doesn't correspond to the one in the selected page above. First thing to take in account is the fact that the first item in a list is at index 0. So when we write `page = pdf.pages[101]`, in fact it is the page 102 which is extracted.\n",
    "But it is still not ok : the extracted text correpond to page 62 which means page 1 in the pdf is actually the page 41 (index 40). The reason is all the \"About\", \"Introduction\", etc. sections are not numbered the same way in the pdf file.\n",
    "This is something to take in account to extract the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target relevant text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents can have several information which are not relevant to build a RAG :\n",
    "* headers and footers ;\n",
    "* tables ;\n",
    "* hyperlink ;\n",
    "* figures ;\n",
    "* etc..\n",
    "\n",
    "We only want to keep the body of the document but also the code samples even if a part of this last is not always relevant. Since each document is different, there is not a unique method to determine what is relevant or not. The only way to handle this is to take time to inspect the document structure, layout, etc..\n",
    "\n",
    "In my case, it appears that the **font** will be the best way to help me target the body and the code.\n",
    "\n",
    "> Take note that working with fonts means we will extract the text character by character to access its properties thanks to the [`chars` object](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#objects) available for each instance of `pdfplumber.PDF` and `pdfplumber.Page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Header fontname : \tGHSRZR+UniversLTStd\n",
      "* Body fontname : \tGHSRZR+SabonLTStd-Roman\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[43]\n",
    "    header_font = page.chars[3].get(\"fontname\")\n",
    "    body_font = page.chars[103].get(\"fontname\")\n",
    "    print(f\"* Header fontname : \\t{header_font}\\n* Body fontname : \\t{body_font}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample code fontname : \tGHSRZR+SourceCodePro-Regular\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[50]\n",
    "    code_font = page.extract_text_lines(return_chars=True)[8][\"chars\"][0].get(\n",
    "        \"fontname\"\n",
    "    )\n",
    "    print(f\"* Sample code fontname : \\t{code_font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header, body and code have different fonts which is of great help. The last thing to take care of is the fact that the text we want to target can be *italic* or **bold**. So let's make a list of all available fonts in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all fonts in the document\n",
    "fontname_list = []\n",
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        [\n",
    "            fontname_list.append(char.get(\"fontname\"))\n",
    "            for char in page.chars\n",
    "            if char.get(\"fontname\") not in fontname_list\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GHSRZR+SabonLTStd-Roman', 'GHSRZR+SourceCodePro-Regular', 'GHSRZR+SabonLTStd-Bold', 'GHSRZR+SabonLTStd-Italic', 'URTXBU+SourceCodePro-Bold']\n"
     ]
    }
   ],
   "source": [
    "# List only the necessary fonts\n",
    "body_fontname_list = [\n",
    "    fontname\n",
    "    for fontname in fontname_list\n",
    "    if \"Sabon\" in fontname or \"SourceCode\" in fontname\n",
    "]\n",
    "print(body_fontname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step for the font part : we will create a helper function to filter the extracted text by font using the fontname of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font filter helper funtion\n",
    "def filter_text_by_font(chars: list[dict], target_fonts: list[str]) -> str:\n",
    "    \"\"\"Filters extracted text and, more precisely, its letters by their fonts.\n",
    "\n",
    "    Args:\n",
    "        chars (list[dict]): chars object from pdfplumber.\n",
    "        target_fonts (list[str]): list of fontnames for which we want to keep the characters/text.\n",
    "\n",
    "    Returns:\n",
    "        str: filtered text.\n",
    "    \"\"\"\n",
    "    char_text = [char[\"text\"] for char in chars if char.get(\"fontname\") in target_fonts]\n",
    "    text = \"\".join(char_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to have the cleanest text as possible for further steps. We will remove uppercase and unecessary spaces. In addition to that, we will also replace *fifi* string by *fi*. This is a specific error I noticed after the extraction of my document which shows how important it is to inspect each document carefully to identify the best way to process it.\n",
    "Here is a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text formatter function\n",
    "def basic_text_formatter(text: str) -> str:\n",
    "    \"\"\"Applies different operations to format and clean the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: formatted text.\n",
    "    \"\"\"\n",
    "    formatted_text = \" \".join(\n",
    "        text.casefold().replace(\"\\n\", \" \").replace(\"fifi\", \"fi\").split()\n",
    "    )\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \t I'm a Basic   text sample. \n",
      "* After : \ti'm a basic text sample.\n"
     ]
    }
   ],
   "source": [
    "basic_text_sample = \" I'm a Basic   text sample. \"\n",
    "\n",
    "print(\n",
    "    f\"* Before : \\t{basic_text_sample}\\n* After : \\t{basic_text_formatter(basic_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens are used to break words so that the appearance of the page is nicer but it will interfere in the words recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con-\n",
      "necting\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[237]\n",
    "    text = page.extract_text()\n",
    "    hyphen_text_sample = text[1066:1078]\n",
    "    print(hyphen_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(text: str) -> str:\n",
    "    \"\"\"Removes hyphens from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: processed text.\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # Find dashes\n",
    "    line_numbers = []\n",
    "    for line_no, line in enumerate(lines[:-1]):\n",
    "        if line.endswith(\"-\"):\n",
    "            line_numbers.append(line_no)\n",
    "\n",
    "    # Replace\n",
    "    for line_no in line_numbers:\n",
    "        lines = dehyphenate(lines, line_no)\n",
    "\n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "def dehyphenate(lines: list[str], line_no: int) -> list[str]:\n",
    "    \"\"\"Rebuilds lines (words) separated by hyphen.\n",
    "\n",
    "    Args:\n",
    "        lines (list[str]): lines to process.\n",
    "        line_no (int): index of lines to process.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of modified lines.\n",
    "    \"\"\"\n",
    "    next_line = lines[line_no + 1]\n",
    "    word_suffix = next_line.split(\" \")[0]\n",
    "\n",
    "    lines[line_no] = lines[line_no][:-1] + word_suffix\n",
    "    lines[line_no + 1] = lines[line_no + 1][len(word_suffix) :]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \tcon-\n",
      "necting\n",
      "* After : \tconnecting \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"* Before : \\t{hyphen_text_sample}\\n* After : \\t{remove_hyphens(hyphen_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our \"tools\"\" to extract the pdf pages correctly and in a relevant way. To refine a bit more our target will remove the pages we don't want to keep (like introduction, glossary, etc.) and we will skip the blank pages (with no content).\n",
    "\n",
    "Do do this, We will write a final function to process our whole document. Pages will be stored in a list of dictionnaries where we will be able to add information like page number, number of characters, tokens, sentences, etc.. and to explore the pages information by converting it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_pdf(path: Path) -> list[dict]:\n",
    "    \"\"\"Open a pdf file with pdfplumber, extracts and formats relevant pages then append\n",
    "    their content and statistics in a list.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Pathlib path of the document.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Extracted content and informations of pages.\n",
    "    \"\"\"\n",
    "    extracted_pages = []\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            page_number = page_idx - 39\n",
    "            lines = page.extract_text_lines(return_chars=True, keep_blank_chars=True)\n",
    "\n",
    "            kept_lines = []\n",
    "            for line in lines:\n",
    "                kept_lines.append(\n",
    "                    filter_text_by_font(line[\"chars\"], body_fontname_list)\n",
    "                )\n",
    "            text = \"\\n\".join(kept_lines)\n",
    "\n",
    "            text = remove_hyphens(text)\n",
    "            text = basic_text_formatter(text)\n",
    "\n",
    "            if 0 < page_number <= 305 and text:\n",
    "                extracted_pages.append(\n",
    "                    {\n",
    "                        \"page_number\": page_number,\n",
    "                        \"page_chars_count\": len(text),\n",
    "                        \"page_words_count\": len(text.split(\" \")),\n",
    "                        \"page_raw_sentences_count\": len(re.split(r\"[.?!]\", text)),\n",
    "                        \"page_text\": text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return extracted_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process pdf\n",
    "extracted_pages = extract_and_process_pdf(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting pages text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we must take in account the fact that we will use the `sentence-transformers` model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) which as *a max tokens capacity of **514***. The token capacity *is very a important concept for a model* since it refers to *the maximum number of tokens it can process* in its context window during a single interaction.\n",
    "\n",
    "In our case, the `all-mpnet-base-v2` model has been trained to ingest and turn into embeddings texts with 514 tokens. Texts over 514 tokens which are encoded by this model will be automatically reduced to 514 tokens in length, potentially losing some information.\n",
    "\n",
    "So what we want to know is how many tokens we have per page. We wil start by a raw tokens counts using the method explained [here](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) or [here](https://python.langchain.com/docs/concepts/tokens/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count\n",
       "mean           2082.93            331.97                     21.32\n",
       "min             121.00             22.00                      1.00\n",
       "max            3705.00            624.00                     55.00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the extracted pages dictionnaries to DataFrame\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[[\"mean\", \"min\", \"max\"]].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know add the `raw_token_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  \n",
       "mean                 520.38  \n",
       "50%                  540.00  \n",
       "75%                  653.00  \n",
       "min                   30.00  \n",
       "max                  926.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate raw tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_raw_tokens_count\"] = len(page[\"page_text\"]) // 4\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can see that the average raw count per page is above the tokens capacity of our model. But is only a raw count so let's try to be more precise.\n",
    "\n",
    "For the next step to come, we will use the *LangChain* framework and its different tools. We will instantiate a `SentenceTransformersTokenTextSplitter` and use the `count_token` method. What is nice is the fact that it is a specialized text splitter for use with `sentence-transformer` models. This means it will behave taking in account the model we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \n",
       "mean                 520.38                  431.07  \n",
       "50%                  540.00                  436.00  \n",
       "75%                  653.00                  544.00  \n",
       "min                   30.00                   29.00  \n",
       "max                  926.00                  785.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Calculate real tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_real_tokens_count\"] = text_splitter.count_tokens(text=page[\"page_text\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. The average tokens count is below the capacity of the model but *we still have 25 % of pages with more than 544 tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chunks/split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to split our text in manageable chunks with the right amount of token. And will to that with using the `split_text` method of `SentenceTransformersTokenTextSplitter`.\n",
    "\n",
    "When splitting the text, we will configure a chunk overlap which define the number of characters which overlap between chunks ensuring that context is preserved. Take in mind that increasing the overlap will increase the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "      <th>page_chunks_max_tokens_count</th>\n",
       "      <th>page_chunks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "      <td>344.08</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \\\n",
       "mean                 520.38                  431.07   \n",
       "50%                  540.00                  436.00   \n",
       "75%                  653.00                  544.00   \n",
       "min                   30.00                   29.00   \n",
       "max                  926.00                  785.00   \n",
       "\n",
       "      page_chunks_max_tokens_count  page_chunks_count  \n",
       "mean                        344.08               2.38  \n",
       "50%                         386.00               2.00  \n",
       "75%                         386.00               3.00  \n",
       "min                          29.00               1.00  \n",
       "max                         389.00               6.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=300, model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Split the text for each page\n",
    "for page in extracted_pages:\n",
    "    page[\"page_chunks\"] = text_splitter.split_text(text=page[\"page_text\"])\n",
    "    page[\"page_chunks_max_tokens_count\"] = max(\n",
    "        text_splitter.count_tokens(text=chunk) for chunk in page[\"page_chunks\"]\n",
    "    )\n",
    "    page[\"page_chunks_count\"] = len(page[\"page_chunks\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice the number of tokens now fits with the model capacity, we are ready to embed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available : device set to CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    device = \"cuda\"\n",
    "    print(f\"CUDA is available : device set to {device.upper()}.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"CUDA is not available : device set to {device.upper()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(pages_dict: list[dict]) -> list[dict]:\n",
    "    \"\"\"Calculate embeddings for chunks of text in each page using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        pages_dict (list[dict]): A list of dictionaries where each dictionary represents a page\n",
    "        with its text chunks and metadata.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary contains the source page number,\n",
    "        the text chunk, and its corresponding embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate embeddings for each chunk of text for each page\n",
    "    embeddings = []\n",
    "\n",
    "    for page in extracted_pages:\n",
    "        for chunk in page[\"page_chunks\"]:\n",
    "            embedding = embedding_model.encode(\n",
    "                sentences=chunk, batch_size=32, device=device, normalize_embeddings=True\n",
    "            )\n",
    "            embeddings.append(\n",
    "                {\n",
    "                    \"source_id\": page[\"page_number\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file found : '/home/anquetos/gcp-professional-data-engineer-rag/datasets/embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings file is available and calculate embeddings if not\n",
    "if not embeddings_filepath.is_file():\n",
    "    print(\"Embeddings file not found, calculating embeddings...\")\n",
    "    # Calculate embeddings\n",
    "    embeddings = calculate_embeddings(extracted_pages)\n",
    "    # Save embeddings to a CSV file\n",
    "    pd.DataFrame(embeddings).to_csv(embeddings_filepath, index=False)\n",
    "else:\n",
    "    print(f\"Embeddings file found : '{embeddings_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>data engineers choose how to store data for ma...</td>\n",
       "      <td>[0.035572313, 0.03913909, -0.028386826, -0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>the data lifecycle consists of four stages : i...</td>\n",
       "      <td>[0.039683174, -0.013291235, -0.04148462, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>with transforming data into a usable format fo...</td>\n",
       "      <td>[0.008390116, -0.016996955, -0.04778342, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>streaming data is a set of data that is typica...</td>\n",
       "      <td>[-0.009797745, -0.042565953, -0.023716504, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>, and pressure data every minute a customer ad...</td>\n",
       "      <td>[-0.0065062963, -0.045118842, -0.01891298, 8.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id                                               text  \\\n",
       "0          2  data engineers choose how to store data for ma...   \n",
       "1          3  the data lifecycle consists of four stages : i...   \n",
       "2          3  with transforming data into a usable format fo...   \n",
       "3          4  streaming data is a set of data that is typica...   \n",
       "4          4  , and pressure data every minute a customer ad...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.035572313, 0.03913909, -0.028386826, -0.036...  \n",
       "1  [0.039683174, -0.013291235, -0.04148462, -0.01...  \n",
       "2  [0.008390116, -0.016996955, -0.04778342, -0.00...  \n",
       "3  [-0.009797745, -0.042565953, -0.023716504, -0....  \n",
       "4  [-0.0065062963, -0.045118842, -0.01891298, 8.3...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load embeddings from the CSV file\n",
    "df_embeddings = pd.read_csv(embeddings_filepath)\n",
    "# Convert the string representation of the embeddings back to numpy 'float32' arrays\n",
    "# (original output format from SentenceTransformer encode method)\n",
    "df_embeddings[\"embedding\"] = df_embeddings[\"embedding\"].apply(\n",
    "    lambda x: np.array(x.strip(\"[]\").split(), dtype=\"float32\")\n",
    ")\n",
    "\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R.A.G."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Tensor shape : torch.Size([677, 768])\n",
      "* Tensor type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Convert the embeddings to PyTorch tensors\n",
    "vectors_tensor = torch.from_numpy(np.stack(df_embeddings[\"embedding\"].values))\n",
    "\n",
    "# Check the shape and type of the tensor\n",
    "print(f\"* Tensor shape : {vectors_tensor.shape}\")\n",
    "print(f\"* Tensor type : {vectors_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def search_top_k_vectors(query: str, k: int = 5) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Search for the top k most similar vectors to a query in the embeddings space.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "        k (int, optional): The number of most similar vectors to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: The top k vectors and their corresponding scores.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = embedding_model.encode(\n",
    "        sentences=query, batch_size=32, device=device, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Calculate the dot product similarity between the query embedding and all the embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=vectors_tensor)\n",
    "\n",
    "    # Get the top k most similar vectors\n",
    "    top_k_vectors = torch.topk(dot_scores[0], k=k)\n",
    "\n",
    "    return top_k_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_sources(query: str, embeddings: pd.DataFrame, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve the top k most relevant sources for a given query based on their embeddings.\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        embeddings (pd.DataFrame): A DataFrame containing the embeddings and associated metadata.\n",
    "        k (int, optional): The number of top results to return. Defaults to 5.\n",
    "    Returns:\n",
    "        str: A formatted string containing the query, scores, source IDs, and corresponding texts of the top k results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the top k most similar vectors\n",
    "    top_k = search_top_k_vectors(query=query, k=k)\n",
    "\n",
    "    # Set the datraframe to use\n",
    "    df = embeddings\n",
    "\n",
    "    # Initialize the results text\n",
    "    results_text = \"\"\n",
    "\n",
    "    # Loop through the top k results and format the output\n",
    "    for score, idx in zip(top_k[0], top_k[1]):\n",
    "        source_id = df.loc[idx.item()][\"source_id\"]\n",
    "        query_text = f\"Query : {query}\\n\"\n",
    "        score_text = f\"Score : {score.item():.4f}\\n\"\n",
    "        source_id_text = f\"Id : {source_id}\\n\"\n",
    "        main_text = (\n",
    "            f\"\"\"{(\" \".join(df.loc[df[\"source_id\"] == source_id, \"text\"].values))}\"\"\"\n",
    "        )\n",
    "        results_text += (\n",
    "            query_text\n",
    "            + score_text\n",
    "            + source_id_text\n",
    "            + textwrap.fill(main_text, width=100)\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    return results_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : 'search_top_k_vectors' function executed in 0.06451 seconds.\n",
      "---\n",
      "Query : CI/CD Practices\n",
      "Score : 0.3481\n",
      "Id : 161\n",
      "you can find the answers in the appendix. you have been tasked with creating a pilot project in gcp\n",
      "to demonstrate the feasibility of migrating workloads from an on - premises hadoop cluster to cloud\n",
      "dataproc. three other engineers will work with you. none of the data that you will use contains\n",
      "sensitive information. you want to minimize the amount of time that you spend on administering the\n",
      "development environment. what would you use to control access to resources in the development\n",
      "environment? predefined roles custom roles primitive roles access control lists the auditors for\n",
      "your company have determined that several employees have more permissions than needed to carry out\n",
      "their job responsibilities. all the employees have users accounts on gcp that have been assigned\n",
      "predefined roles. you have concluded that the optimal way to meet the auditors ’ recommendations is\n",
      "by using custom roles. what permission is needed to create a custom role? iam. roles. create iam.\n",
      "custom. roles roles / iam. custom. create roles / iam. create. custom you have created a managed\n",
      "instance group in compute engine to run a high - performance computing application. the application\n",
      "will read source data from a cloud storage bucket and write results to another bucket. the\n",
      "application will run whenever new data is uploaded to cloud storage via a cloud function that\n",
      "invokes the script to start the job. you will need to assign the role roles / storage. objectcreator\n",
      "to an identity so that the application can write the output data to cloud storage. to what kind of\n",
      "identity would you assign the roles? user. group. service account. you wouldn ’ t. the role would be\n",
      "assigned to the bucket. your company has implemented an organizational hierarchy consisting of two\n",
      "layers of folders and tens of projects. the top layer of folders corresponds to a department, and\n",
      "the second layer of folders are working groups within a department. each working group has one or\n",
      "use to control access to resources in the development environment? predefined roles custom roles\n",
      "primitive roles access control lists the auditors for your company have determined that several\n",
      "employees have more permissions than needed to carry out their job responsibilities. all the\n",
      "employees have users accounts on gcp that have been assigned predefined roles. you have concluded\n",
      "that the optimal way to meet the auditors ’ recommendations is by using custom roles. what\n",
      "permission is needed to create a custom role? iam. roles. create iam. custom. roles roles / iam.\n",
      "custom. create roles / iam. create. custom you have created a managed instance group in compute\n",
      "engine to run a high - performance computing application. the application will read source data from\n",
      "a cloud storage bucket and write results to another bucket. the application will run whenever new\n",
      "data is uploaded to cloud storage via a cloud function that invokes the script to start the job. you\n",
      "will need to assign the role roles / storage. objectcreator to an identity so that the application\n",
      "can write the output data to cloud storage. to what kind of identity would you assign the roles?\n",
      "user. group. service account. you wouldn ’ t. the role would be assigned to the bucket. your company\n",
      "has implemented an organizational hierarchy consisting of two layers of folders and tens of\n",
      "projects. the top layer of folders corresponds to a department, and the second layer of folders are\n",
      "working groups within a department. each working group has one or more projects in the resource\n",
      "hierarchy. you have to ensure that all projects comply with regulations, so you have created several\n",
      "policies. policy a applies to all departments.\n",
      "\n",
      "Query : CI/CD Practices\n",
      "Score : 0.3259\n",
      "Id : 291\n",
      "c. the correct answer is c. use cloud dataflflow templates to specify the pattern and provide\n",
      "parameters for users to customize the template. option a is incorrect since this would require users\n",
      "to customize the code in the script. options b and d are incorrect because cloud dataproc should not\n",
      "be used for this requirement. also, option d is incorrect because there are no cloud dataproc\n",
      "templates. d. the correct answer is d. you should create an ephemeral cluster for each job and\n",
      "delete the cluster after the job completes. option a is incorrect because that is a more complicated\n",
      "configuration. option b is incorrect because it keeps the cluster running instead of shutting down\n",
      "after jobs complete. option c is incorrect because it keeps the clusters running after the jobs\n",
      "complete. a. the correct answer is a, cloud composer, which is designed to support workflflow\n",
      "orchestration. options b and c are incorrect because they are both implementations of the apache\n",
      "beam model that is used for executing stream and batch processing program. option d is incorrect ;\n",
      "cloud dataproc is a managed hadoop and spark service. b. the correct answer is b. the data could be\n",
      "stored in cloud bigtable, which provides consistent, scalable performance. option a is incorrect\n",
      "because cloud storage is an object storage system, not a database. option c is incorrect, since\n",
      "cloud datastore is a documentstyle nosql database and is not suitable for a data warehouse. option d\n",
      "is incorrect ; cloud dataflflow is not a database. d. the correct answer is d. with change data\n",
      "capture, each change is a source system captured and recorded in a data store. options a, b, and c\n",
      "all capture the state of source systems at a point in time and do not capture changes between those\n",
      "times. b. the correct answer is b. iot sensors can write data to a cloud pub / sub d. the correct\n",
      "answer is d. you should create an ephemeral cluster for each job and delete the cluster after the\n",
      "job completes. option a is incorrect because that is a more complicated configuration. option b is\n",
      "incorrect because it keeps the cluster running instead of shutting down after jobs complete. option\n",
      "c is incorrect because it keeps the clusters running after the jobs complete. a. the correct answer\n",
      "is a, cloud composer, which is designed to support workflflow orchestration. options b and c are\n",
      "incorrect because they are both implementations of the apache beam model that is used for executing\n",
      "stream and batch processing program. option d is incorrect ; cloud dataproc is a managed hadoop and\n",
      "spark service. b. the correct answer is b. the data could be stored in cloud bigtable, which\n",
      "provides consistent, scalable performance. option a is incorrect because cloud storage is an object\n",
      "storage system, not a database. option c is incorrect, since cloud datastore is a documentstyle\n",
      "nosql database and is not suitable for a data warehouse. option d is incorrect ; cloud dataflflow is\n",
      "not a database. d. the correct answer is d. with change data capture, each change is a source system\n",
      "captured and recorded in a data store. options a, b, and c all capture the state of source systems\n",
      "at a point in time and do not capture changes between those times. b. the correct answer is b. iot\n",
      "sensors can write data to a cloud pub / sub topic. when a message is written, it can trigger a cloud\n",
      "function that runs the associated code. cloud functions can execute the python validation check, and\n",
      "if the validation check fails, the message is removed from the queue. option a is incorrect ; cloud\n",
      "storage is not a for streaming ingestion. option c is incorrect because bigquery is an analytical\n",
      "database that could be used in later stages but not during ingest , which is designed to support\n",
      "workflflow orchestration. options b and c are incorrect because they are both implementations of the\n",
      "apache beam model that is used for executing stream and batch processing program. option d is\n",
      "incorrect ; cloud dataproc is a managed hadoop and spark service. b. the correct answer is b. the\n",
      "data could be stored in cloud bigtable, which provides consistent, scalable performance. option a is\n",
      "incorrect because cloud storage is an object storage system, not a database. option c is incorrect,\n",
      "since cloud datastore is a documentstyle nosql database and is not suitable for a data warehouse.\n",
      "option d is incorrect ; cloud dataflflow is not a database. d. the correct answer is d. with change\n",
      "data capture, each change is a source system captured and recorded in a data store. options a, b,\n",
      "and c all capture the state of source systems at a point in time and do not capture changes between\n",
      "those times. b. the correct answer is b. iot sensors can write data to a cloud pub / sub topic. when\n",
      "a message is written, it can trigger a cloud function that runs the associated code. cloud functions\n",
      "can execute the python validation check, and if the validation check fails, the message is removed\n",
      "from the queue. option a is incorrect ; cloud storage is not a for streaming ingestion. option c is\n",
      "incorrect because bigquery is an analytical database that could be used in later stages but not\n",
      "during ingest. answer d is incorrect because cloud storage is not a suitable choice for high -\n",
      "volume streaming ingestion, and bigquery is not suitable for storing data during ingestion. a. the\n",
      "answer is a. this scenario calls for full control over the choice of the operating system, and the\n",
      "application is moving from a physical server so that it is not containerized. compute engine can run\n",
      "the application in a v a is incorrect because cloud storage is an object storage system, not a\n",
      "database. option c is incorrect, since cloud datastore is a documentstyle nosql database and is not\n",
      "suitable for a data warehouse. option d is incorrect ; cloud dataflflow is not a database. d. the\n",
      "correct answer is d. with change data capture, each change is a source system captured and recorded\n",
      "in a data store. options a, b, and c all capture the state of source systems at a point in time and\n",
      "do not capture changes between those times. b. the correct answer is b. iot sensors can write data\n",
      "to a cloud pub / sub topic. when a message is written, it can trigger a cloud function that runs the\n",
      "associated code. cloud functions can execute the python validation check, and if the validation\n",
      "check fails, the message is removed from the queue. option a is incorrect ; cloud storage is not a\n",
      "for streaming ingestion. option c is incorrect because bigquery is an analytical database that could\n",
      "be used in later stages but not during ingest. answer d is incorrect because cloud storage is not a\n",
      "suitable choice for high - volume streaming ingestion, and bigquery is not suitable for storing data\n",
      "during ingestion. a. the answer is a. this scenario calls for full control over the choice of the\n",
      "operating system, and the application is moving from a physical server so that it is not\n",
      "containerized. compute engine can run the application in a vm configured with ubuntu 14. 04 and the\n",
      "additional packages. option b is incorrect because the application is not containerized ( although\n",
      "it may be modified to be containerized ). option c is incorrect because the application cannot run\n",
      "in one of the language - specific runtimes of app engine standard. option d is incorrect because the\n",
      "cloud functions product runs code in response to events and does not support long - running options\n",
      "a, b, and c all capture the state of source systems at a point in time and do not capture changes\n",
      "between those times. b. the correct answer is b. iot sensors can write data to a cloud pub / sub\n",
      "topic. when a message is written, it can trigger a cloud function that runs the associated code.\n",
      "cloud functions can execute the python validation check, and if the validation check fails, the\n",
      "message is removed from the queue. option a is incorrect ; cloud storage is not a for streaming\n",
      "ingestion. option c is incorrect because bigquery is an analytical database that could be used in\n",
      "later stages but not during ingest. answer d is incorrect because cloud storage is not a suitable\n",
      "choice for high - volume streaming ingestion, and bigquery is not suitable for storing data during\n",
      "ingestion. a. the answer is a. this scenario calls for full control over the choice of the operating\n",
      "system, and the application is moving from a physical server so that it is not containerized.\n",
      "compute engine can run the application in a vm configured with ubuntu 14. 04 and the additional\n",
      "packages. option b is incorrect because the application is not containerized ( although it may be\n",
      "modified to be containerized ). option c is incorrect because the application cannot run in one of\n",
      "the language - specific runtimes of app engine standard. option d is incorrect because the cloud\n",
      "functions product runs code in response to events and does not support long - running applications.\n",
      "\n",
      "Query : CI/CD Practices\n",
      "Score : 0.3216\n",
      "Id : 161\n",
      "you can find the answers in the appendix. you have been tasked with creating a pilot project in gcp\n",
      "to demonstrate the feasibility of migrating workloads from an on - premises hadoop cluster to cloud\n",
      "dataproc. three other engineers will work with you. none of the data that you will use contains\n",
      "sensitive information. you want to minimize the amount of time that you spend on administering the\n",
      "development environment. what would you use to control access to resources in the development\n",
      "environment? predefined roles custom roles primitive roles access control lists the auditors for\n",
      "your company have determined that several employees have more permissions than needed to carry out\n",
      "their job responsibilities. all the employees have users accounts on gcp that have been assigned\n",
      "predefined roles. you have concluded that the optimal way to meet the auditors ’ recommendations is\n",
      "by using custom roles. what permission is needed to create a custom role? iam. roles. create iam.\n",
      "custom. roles roles / iam. custom. create roles / iam. create. custom you have created a managed\n",
      "instance group in compute engine to run a high - performance computing application. the application\n",
      "will read source data from a cloud storage bucket and write results to another bucket. the\n",
      "application will run whenever new data is uploaded to cloud storage via a cloud function that\n",
      "invokes the script to start the job. you will need to assign the role roles / storage. objectcreator\n",
      "to an identity so that the application can write the output data to cloud storage. to what kind of\n",
      "identity would you assign the roles? user. group. service account. you wouldn ’ t. the role would be\n",
      "assigned to the bucket. your company has implemented an organizational hierarchy consisting of two\n",
      "layers of folders and tens of projects. the top layer of folders corresponds to a department, and\n",
      "the second layer of folders are working groups within a department. each working group has one or\n",
      "use to control access to resources in the development environment? predefined roles custom roles\n",
      "primitive roles access control lists the auditors for your company have determined that several\n",
      "employees have more permissions than needed to carry out their job responsibilities. all the\n",
      "employees have users accounts on gcp that have been assigned predefined roles. you have concluded\n",
      "that the optimal way to meet the auditors ’ recommendations is by using custom roles. what\n",
      "permission is needed to create a custom role? iam. roles. create iam. custom. roles roles / iam.\n",
      "custom. create roles / iam. create. custom you have created a managed instance group in compute\n",
      "engine to run a high - performance computing application. the application will read source data from\n",
      "a cloud storage bucket and write results to another bucket. the application will run whenever new\n",
      "data is uploaded to cloud storage via a cloud function that invokes the script to start the job. you\n",
      "will need to assign the role roles / storage. objectcreator to an identity so that the application\n",
      "can write the output data to cloud storage. to what kind of identity would you assign the roles?\n",
      "user. group. service account. you wouldn ’ t. the role would be assigned to the bucket. your company\n",
      "has implemented an organizational hierarchy consisting of two layers of folders and tens of\n",
      "projects. the top layer of folders corresponds to a department, and the second layer of folders are\n",
      "working groups within a department. each working group has one or more projects in the resource\n",
      "hierarchy. you have to ensure that all projects comply with regulations, so you have created several\n",
      "policies. policy a applies to all departments.\n",
      "\n",
      "Query : CI/CD Practices\n",
      "Score : 0.3215\n",
      "Id : 30\n",
      "not only are data engineers expected to understand when to use various ingestion and storage\n",
      "technologies, but they should also be familiar with deploying and operating those systems. in this\n",
      "chapter, you will learn how to deploy storage systems and perform data management operations, such\n",
      "as importing and exporting data, confi guring access controls, and doing performance tuning. the\n",
      "services included in this chapter are as follows : cloud sql cloud spanner cloud bigtable cloud\n",
      "firestore bigquery cloud memorystore cloud storage the chapter also includes a discussion of working\n",
      "with unmanaged databases, storage costs and performance, and data lifecycle management. cloud sql is\n",
      "a fully managed relational database service that supports mysql, postgresql, and sql server\n",
      "databases. as of this writing, sql server is in beta release. a managed database is one that does\n",
      "not require as much administration and operational support as an unmanaged database because google\n",
      "will take care of core operational tasks, such as creating databases, performing backups, and\n",
      "updating the operating system of database instances. google also manages scaling disks, confi guring\n",
      "for failover, monitoring, and authorizing network connections. cloud sql supports regional - level\n",
      "databases of up to 30 tb. if you need to store more data or need multi - regional support, consider\n",
      "using cloud spanner.\n",
      "\n",
      "Query : CI/CD Practices\n",
      "Score : 0.3193\n",
      "Id : 140\n",
      "since data engineers work with diverse sets of data, they will likely need to use a variety of data\n",
      "stores that use access controls. they also should be prepared to work with sensitive data that needs\n",
      "additional protections. this chapter introduces several key topics of security and compliance,\n",
      "including identity and access management data security, including encryption and key management data\n",
      "loss prevention compliance we ’ ll begin with identity and access management, because it is\n",
      "fundamental to many security practices. cloud iam is google cloud ’ s fine - grained identity and\n",
      "access management service that is used to control which users can perform operations on resources\n",
      "within gcp. cloud iam uses the concept of roles, which are collections of permissions that can be\n",
      "assigned to identities. permissions are granted to roles, and then individuals can be assigned\n",
      "multiple roles to gain these permissions. cloud iam provides a large number of roles tuned to common\n",
      "use cases, such as server administrators or database operators. along with roles, additional\n",
      "attributes about resources or identities, such as ip address and date and time, can be considered\n",
      "when making access control decisions ; this is known as context - aware access. cloud iam maintains\n",
      "an audit log of changes to permissions, including authorizing, removing, and delegating permissions.\n",
      "this chapter describes key aspects of cloud iam that you should understand when taking the\n",
      "professional data engineer exam, including predefined roles custom roles\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search using the indicated query\n",
    "result = retrieve_relevant_sources(\n",
    "    \"CI/CD Practices\", df_embeddings, 5\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
