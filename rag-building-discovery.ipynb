{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pdf exploration and preparation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main goal is to check if we can read the pdf, extract only relevant content, see how we can post-process the extracted text and finally have some informations about the extracted text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to `Simple Local RAG Tutorial` :\n",
    "* [GitHub](https://github.com/mrdbourke/simple-local-rag) ;\n",
    "* [YouTube](https://youtu.be/qN_2fnOPY-M?si=APnkpsGY0z_scJ9Z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "from utils.timing_functions import timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set files path for pdf document and embeddings\n",
    "PDF_FILENAME = \"source.pdf\"\n",
    "EMBEDDINGS_FILENAME = \"embeddings.csv\"\n",
    "\n",
    "p = Path()\n",
    "\n",
    "pdf_filepath = p.resolve() / \"pdf\" / PDF_FILENAME\n",
    "embeddings_filepath = p.resolve() / \"datasets\" / EMBEDDINGS_FILENAME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the pdf pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can verify that our source pdf is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pdf file path : '/home/anquetos/gcp-professional-data-engineer-rag/pdf/source.pdf'.\n"
     ]
    }
   ],
   "source": [
    "# Check if pdf file is available\n",
    "if pdf_filepath.is_file():\n",
    "    print(f\"Pdf file path : '{pdf_filepath}'.\")\n",
    "else:\n",
    "    print(\"No pdf file found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the number of pages found is the right one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Expected number of pages : \t355\n",
      "* Number of pages found : \t355\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    print(\n",
    "        f\"* Expected number of pages : \\t355\\n* Number of pages found : \\t{len(pdf.pages)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's ok, we can try to extract text from a random test page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipelines are sequences of operations that copy, trans-\n",
      "form, load, and analyze data.\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[101]\n",
    "    text = page.extract_text()\n",
    "    print(text[:90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction works but the text doesn't correspond to the one in the selected page above. First thing to take in account is the fact that the first item in a list is at index 0. So when we write `page = pdf.pages[101]`, in fact it is the page 102 which is extracted.\n",
    "But it is still not ok : the extracted text correpond to page 62 which means page 1 in the pdf is actually the page 41 (index 40). The reason is all the \"About\", \"Introduction\", etc. sections are not numbered the same way in the pdf file.\n",
    "This is something to take in account to extract the desired content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target relevant text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents can have several information which are not relevant to build a RAG :\n",
    "* headers and footers ;\n",
    "* tables ;\n",
    "* hyperlink ;\n",
    "* figures ;\n",
    "* etc..\n",
    "\n",
    "We only want to keep the body of the document but also the code samples even if a part of this last is not always relevant. Since each document is different, there is not a unique method to determine what is relevant or not. The only way to handle this is to take time to inspect the document structure, layout, etc..\n",
    "\n",
    "In my case, it appears that the **font** will be the best way to help me target the body and the code.\n",
    "\n",
    "> Take note that working with fonts means we will extract the text character by character to access its properties thanks to the [`chars` object](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#objects) available for each instance of `pdfplumber.PDF` and `pdfplumber.Page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Header fontname : \tGHSRZR+UniversLTStd\n",
      "* Body fontname : \tGHSRZR+SabonLTStd-Roman\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[43]\n",
    "    header_font = page.chars[3].get(\"fontname\")\n",
    "    body_font = page.chars[103].get(\"fontname\")\n",
    "    print(f\"* Header fontname : \\t{header_font}\\n* Body fontname : \\t{body_font}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Sample code fontname : \tGHSRZR+SourceCodePro-Regular\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[50]\n",
    "    code_font = page.extract_text_lines(return_chars=True)[8][\"chars\"][0].get(\n",
    "        \"fontname\"\n",
    "    )\n",
    "    print(f\"* Sample code fontname : \\t{code_font}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Header, body and code have different fonts which is of great help. The last thing to take care of is the fact that the text we want to target can be *italic* or **bold**. So let's make a list of all available fonts in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all fonts in the document\n",
    "fontname_list = []\n",
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        [\n",
    "            fontname_list.append(char.get(\"fontname\"))\n",
    "            for char in page.chars\n",
    "            if char.get(\"fontname\") not in fontname_list\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GHSRZR+SabonLTStd-Roman', 'GHSRZR+SourceCodePro-Regular', 'GHSRZR+SabonLTStd-Bold', 'GHSRZR+SabonLTStd-Italic', 'URTXBU+SourceCodePro-Bold']\n"
     ]
    }
   ],
   "source": [
    "# List only the necessary fonts\n",
    "body_fontname_list = [\n",
    "    fontname\n",
    "    for fontname in fontname_list\n",
    "    if \"Sabon\" in fontname or \"SourceCode\" in fontname\n",
    "]\n",
    "print(body_fontname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step for the font part : we will create a helper function to filter the extracted text by font using the fontname of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font filter helper funtion\n",
    "def filter_text_by_font(chars: list[dict], target_fonts: list[str]) -> str:\n",
    "    \"\"\"Filters extracted text and, more precisely, its letters by their fonts.\n",
    "\n",
    "    Args:\n",
    "        chars (list[dict]): chars object from pdfplumber.\n",
    "        target_fonts (list[str]): list of fontnames for which we want to keep the characters/text.\n",
    "\n",
    "    Returns:\n",
    "        str: filtered text.\n",
    "    \"\"\"\n",
    "    char_text = [char[\"text\"] for char in chars if char.get(\"fontname\") in target_fonts]\n",
    "    text = \"\".join(char_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to have the cleanest text as possible for further steps. We will remove uppercase and unecessary spaces. In addition to that, we will also replace *fifi* string by *fi*. This is a specific error I noticed after the extraction of my document which shows how important it is to inspect each document carefully to identify the best way to process it.\n",
    "Here is a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text formatter function\n",
    "def basic_text_formatter(text: str) -> str:\n",
    "    \"\"\"Applies different operations to format and clean the text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: formatted text.\n",
    "    \"\"\"\n",
    "    formatted_text = \" \".join(\n",
    "        text.casefold().replace(\"\\n\", \" \").replace(\"fifi\", \"fi\").split()\n",
    "    )\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \t I'm a Basic   text sample. \n",
      "* After : \ti'm a basic text sample.\n"
     ]
    }
   ],
   "source": [
    "basic_text_sample = \" I'm a Basic   text sample. \"\n",
    "\n",
    "print(\n",
    "    f\"* Before : \\t{basic_text_sample}\\n* After : \\t{basic_text_formatter(basic_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyphens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyphens are used to break words so that the appearance of the page is nicer but it will interfere in the words recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con-\n",
      "necting\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(pdf_filepath) as pdf:\n",
    "    page = pdf.pages[237]\n",
    "    text = page.extract_text()\n",
    "    hyphen_text_sample = text[1066:1078]\n",
    "    print(hyphen_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(text: str) -> str:\n",
    "    \"\"\"Removes hyphens from text.\n",
    "\n",
    "    Args:\n",
    "        text (str): original text.\n",
    "\n",
    "    Returns:\n",
    "        str: processed text.\n",
    "    \"\"\"\n",
    "    lines = [line.rstrip() for line in text.split(\"\\n\")]\n",
    "\n",
    "    # Find dashes\n",
    "    line_numbers = []\n",
    "    for line_no, line in enumerate(lines[:-1]):\n",
    "        if line.endswith(\"-\"):\n",
    "            line_numbers.append(line_no)\n",
    "\n",
    "    # Replace\n",
    "    for line_no in line_numbers:\n",
    "        lines = dehyphenate(lines, line_no)\n",
    "\n",
    "    return \" \".join(lines)\n",
    "\n",
    "\n",
    "def dehyphenate(lines: list[str], line_no: int) -> list[str]:\n",
    "    \"\"\"Rebuilds lines (words) separated by hyphen.\n",
    "\n",
    "    Args:\n",
    "        lines (list[str]): lines to process.\n",
    "        line_no (int): index of lines to process.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: list of modified lines.\n",
    "    \"\"\"\n",
    "    next_line = lines[line_no + 1]\n",
    "    word_suffix = next_line.split(\" \")[0]\n",
    "\n",
    "    lines[line_no] = lines[line_no][:-1] + word_suffix\n",
    "    lines[line_no + 1] = lines[line_no + 1][len(word_suffix) :]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before : \tcon-\n",
      "necting\n",
      "* After : \tconnecting \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"* Before : \\t{hyphen_text_sample}\\n* After : \\t{remove_hyphens(hyphen_text_sample)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our \"tools\"\" to extract the pdf pages correctly and in a relevant way. To refine a bit more our target will remove the pages we don't want to keep (like introduction, glossary, etc.) and we will skip the blank pages (with no content).\n",
    "\n",
    "Do do this, We will write a final function to process our whole document. Pages will be stored in a list of dictionnaries where we will be able to add information like page number, number of characters, tokens, sentences, etc.. and to explore the pages information by converting it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_pdf(path: Path) -> list[dict]:\n",
    "    \"\"\"Open a pdf file with pdfplumber, extracts and formats relevant pages then append\n",
    "    their content and statistics in a list.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Pathlib path of the document.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Extracted content and informations of pages.\n",
    "    \"\"\"\n",
    "    extracted_pages = []\n",
    "\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page_idx, page in enumerate(pdf.pages):\n",
    "            page_number = page_idx - 39\n",
    "            lines = page.extract_text_lines(return_chars=True, keep_blank_chars=True)\n",
    "\n",
    "            kept_lines = []\n",
    "            for line in lines:\n",
    "                kept_lines.append(\n",
    "                    filter_text_by_font(line[\"chars\"], body_fontname_list)\n",
    "                )\n",
    "            text = \"\\n\".join(kept_lines)\n",
    "\n",
    "            text = remove_hyphens(text)\n",
    "            text = basic_text_formatter(text)\n",
    "\n",
    "            if 0 < page_number <= 305 and text:\n",
    "                extracted_pages.append(\n",
    "                    {\n",
    "                        \"page_number\": page_number,\n",
    "                        \"page_chars_count\": len(text),\n",
    "                        \"page_words_count\": len(text.split(\" \")),\n",
    "                        \"page_raw_sentences_count\": len(re.split(r\"[.?!]\", text)),\n",
    "                        \"page_text\": text,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return extracted_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and process pdf\n",
    "extracted_pages = extract_and_process_pdf(pdf_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting pages text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we must take in account the fact that we will use the `sentence-transformers` model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) which as *a max tokens capacity of **514***. The token capacity *is very a important concept for a model* since it refers to *the maximum number of tokens it can process* in its context window during a single interaction.\n",
    "\n",
    "In our case, the `all-mpnet-base-v2` model has been trained to ingest and turn into embeddings texts with 514 tokens. Texts over 514 tokens which are encoded by this model will be automatically reduced to 514 tokens in length, potentially losing some information.\n",
    "\n",
    "So what we want to know is how many tokens we have per page. We wil start by a raw tokens counts using the method explained [here](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) or [here](https://python.langchain.com/docs/concepts/tokens/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count\n",
       "mean           2082.93            331.97                     21.32\n",
       "min             121.00             22.00                      1.00\n",
       "max            3705.00            624.00                     55.00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the extracted pages dictionnaries to DataFrame\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[[\"mean\", \"min\", \"max\"]].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can know add the `raw_token_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  \n",
       "mean                 520.38  \n",
       "50%                  540.00  \n",
       "75%                  653.00  \n",
       "min                   30.00  \n",
       "max                  926.00  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate raw tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_raw_tokens_count\"] = len(page[\"page_text\"]) // 4\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results we can see that the average raw count per page is above the tokens capacity of our model. But is only a raw count so let's try to be more precise.\n",
    "\n",
    "For the next step to come, we will use the *LangChain* framework and its different tools. We will instantiate a `SentenceTransformersTokenTextSplitter` and use the `count_token` method. What is nice is the fact that it is a specialized text splitter for use with `sentence-transformer` models. This means it will behave taking in account the model we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \n",
       "mean                 520.38                  431.07  \n",
       "50%                  540.00                  436.00  \n",
       "75%                  653.00                  544.00  \n",
       "min                   30.00                   29.00  \n",
       "max                  926.00                  785.00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Calculate real tokens count\n",
    "for page in extracted_pages:\n",
    "    page[\"page_real_tokens_count\"] = text_splitter.count_tokens(text=page[\"page_text\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. The average tokens count is below the capacity of the model but *we still have 25 % of pages with more than 544 tokens*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chunks/split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to split our text in manageable chunks with the right amount of token. And will to that with using the `split_text` method of `SentenceTransformersTokenTextSplitter`.\n",
    "\n",
    "When splitting the text, we will configure a chunk overlap which define the number of characters which overlap between chunks ensuring that context is preserved. Take in mind that increasing the overlap will increase the number of chunks created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chars_count</th>\n",
       "      <th>page_words_count</th>\n",
       "      <th>page_raw_sentences_count</th>\n",
       "      <th>page_raw_tokens_count</th>\n",
       "      <th>page_real_tokens_count</th>\n",
       "      <th>page_chunks_max_tokens_count</th>\n",
       "      <th>page_chunks_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2082.93</td>\n",
       "      <td>331.97</td>\n",
       "      <td>21.32</td>\n",
       "      <td>520.38</td>\n",
       "      <td>431.07</td>\n",
       "      <td>344.08</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2161.00</td>\n",
       "      <td>341.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>540.00</td>\n",
       "      <td>436.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2615.00</td>\n",
       "      <td>414.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>653.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>386.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>121.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>29.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3705.00</td>\n",
       "      <td>624.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>926.00</td>\n",
       "      <td>785.00</td>\n",
       "      <td>389.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_chars_count  page_words_count  page_raw_sentences_count  \\\n",
       "mean           2082.93            331.97                     21.32   \n",
       "50%            2161.00            341.00                     20.00   \n",
       "75%            2615.00            414.00                     26.00   \n",
       "min             121.00             22.00                      1.00   \n",
       "max            3705.00            624.00                     55.00   \n",
       "\n",
       "      page_raw_tokens_count  page_real_tokens_count  \\\n",
       "mean                 520.38                  431.07   \n",
       "50%                  540.00                  436.00   \n",
       "75%                  653.00                  544.00   \n",
       "min                   30.00                   29.00   \n",
       "max                  926.00                  785.00   \n",
       "\n",
       "      page_chunks_max_tokens_count  page_chunks_count  \n",
       "mean                        344.08               2.38  \n",
       "50%                         386.00               2.00  \n",
       "75%                         386.00               3.00  \n",
       "min                          29.00               1.00  \n",
       "max                         389.00               6.00  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=300, model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Split the text for each page\n",
    "for page in extracted_pages:\n",
    "    page[\"page_chunks\"] = text_splitter.split_text(text=page[\"page_text\"])\n",
    "    page[\"page_chunks_max_tokens_count\"] = max(\n",
    "        text_splitter.count_tokens(text=chunk) for chunk in page[\"page_chunks\"]\n",
    "    )\n",
    "    page[\"page_chunks_count\"] = len(page[\"page_chunks\"])\n",
    "\n",
    "# Regenerate DataFrame and statistics\n",
    "df = pd.DataFrame(extracted_pages)\n",
    "df.describe().drop(columns=[\"page_number\"]).loc[\n",
    "    [\"mean\", \"50%\", \"75%\", \"min\", \"max\"]\n",
    "].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice the number of tokens now fits with the model capacity, we are ready to embed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available : device set to CUDA.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"CUDA is available : device set to {device.upper()}.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"CUDA is not available : device set to {device.upper()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SentenceTransformer model\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(pages_dict: list[dict]) -> list[dict]:\n",
    "    \"\"\"Calculate embeddings for chunks of text in each page using a SentenceTransformer model.\n",
    "\n",
    "    Args:\n",
    "        pages_dict (list[dict]): A list of dictionaries where each dictionary represents a page\n",
    "        with its text chunks and metadata.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries where each dictionary contains the source page number,\n",
    "        the text chunk, and its corresponding embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate embeddings for each chunk of text for each page\n",
    "    embeddings = []\n",
    "\n",
    "    for page in extracted_pages:\n",
    "        for chunk in page[\"page_chunks\"]:\n",
    "            embedding = embedding_model.encode(\n",
    "                sentences=chunk, batch_size=32, device=device, normalize_embeddings=True\n",
    "            )\n",
    "            embeddings.append(\n",
    "                {\n",
    "                    \"source_id\": page[\"page_number\"],\n",
    "                    \"text\": chunk,\n",
    "                    \"embedding\": embedding,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file found : '/home/anquetos/gcp-professional-data-engineer-rag/datasets/embeddings.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Check if embeddings file is available and calculate embeddings if not\n",
    "if not embeddings_filepath.is_file():\n",
    "    print(\"Embeddings file not found, calculating embeddings...\")\n",
    "    # Calculate embeddings\n",
    "    embeddings = calculate_embeddings(extracted_pages)\n",
    "    # Save embeddings to a CSV file\n",
    "    pd.DataFrame(embeddings).to_csv(embeddings_filepath, index=False)\n",
    "else:\n",
    "    print(f\"Embeddings file found : '{embeddings_filepath}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>data engineers choose how to store data for ma...</td>\n",
       "      <td>[0.035572313, 0.03913909, -0.028386826, -0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>the data lifecycle consists of four stages : i...</td>\n",
       "      <td>[0.039683174, -0.013291235, -0.04148462, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>with transforming data into a usable format fo...</td>\n",
       "      <td>[0.008390116, -0.016996955, -0.04778342, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>streaming data is a set of data that is typica...</td>\n",
       "      <td>[-0.009797745, -0.042565953, -0.023716504, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>, and pressure data every minute a customer ad...</td>\n",
       "      <td>[-0.0065062963, -0.045118842, -0.01891298, 8.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source_id                                               text  \\\n",
       "0          2  data engineers choose how to store data for ma...   \n",
       "1          3  the data lifecycle consists of four stages : i...   \n",
       "2          3  with transforming data into a usable format fo...   \n",
       "3          4  streaming data is a set of data that is typica...   \n",
       "4          4  , and pressure data every minute a customer ad...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.035572313, 0.03913909, -0.028386826, -0.036...  \n",
       "1  [0.039683174, -0.013291235, -0.04148462, -0.01...  \n",
       "2  [0.008390116, -0.016996955, -0.04778342, -0.00...  \n",
       "3  [-0.009797745, -0.042565953, -0.023716504, -0....  \n",
       "4  [-0.0065062963, -0.045118842, -0.01891298, 8.3...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load embeddings from the CSV file\n",
    "df_embeddings = pd.read_csv(embeddings_filepath)\n",
    "# Convert the string representation of the embeddings back to numpy 'float32' arrays\n",
    "# (original output format from SentenceTransformer encode method)\n",
    "df_embeddings[\"embedding\"] = df_embeddings[\"embedding\"].apply(\n",
    "    lambda x: np.array(x.strip(\"[]\").split(), dtype=\"float32\")\n",
    ")\n",
    "\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R.A.G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve information from the source document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Tensor shape : torch.Size([677, 768])\n",
      "* Tensor type : torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Convert the embeddings to PyTorch tensors\n",
    "embeddings_vectors_tensor = torch.from_numpy(\n",
    "    np.stack(df_embeddings[\"embedding\"].values)\n",
    ")\n",
    "\n",
    "# Check the shape and type of the tensor\n",
    "print(f\"* Tensor shape : {embeddings_vectors_tensor.shape}\")\n",
    "print(f\"* Tensor type : {embeddings_vectors_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing\n",
    "def search_top_k_vectors(query: str, k: int = 5) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Search for the top k most similar vectors to a query in the embeddings space.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text.\n",
    "        k (int, optional): The number of most similar vectors to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: The top k vectors and their corresponding scores.\n",
    "    \"\"\"\n",
    "\n",
    "    query_embedding = embedding_model.encode(\n",
    "        sentences=query, batch_size=32, device=device, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Calculate the dot product similarity between the query embedding and all the embeddings\n",
    "    dot_scores = util.dot_score(a=query_embedding, b=embeddings_vectors_tensor)\n",
    "\n",
    "    # Get the top k most similar vectors\n",
    "    top_k_vectors = torch.topk(dot_scores[0], k=k)\n",
    "\n",
    "    return top_k_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_sources(query: str, embeddings: pd.DataFrame, k: int = 5) -> str:\n",
    "    \"\"\"Retrieve the top k most relevant sources for a given query based on their embeddings.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        embeddings (pd.DataFrame): A DataFrame containing the embeddings and associated metadata.\n",
    "        k (int, optional): The number of top results to return. Defaults to 5.\n",
    "    Returns:\n",
    "        str: A formatted string containing the query, scores, source IDs, and corresponding texts of the top k results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the top k most similar vectors\n",
    "    top_k = search_top_k_vectors(query=query, k=k)\n",
    "\n",
    "    # Set the datraframe to use\n",
    "    df = embeddings\n",
    "\n",
    "    # Initialize the results text\n",
    "    results_text = \"\"\n",
    "\n",
    "    # Loop through the top k results and format the output\n",
    "    for score, idx in zip(top_k[0], top_k[1]):\n",
    "        source_id = df.loc[idx.item()][\"source_id\"]\n",
    "        query_text = f\"Query : {query}\\n\"\n",
    "        score_text = f\"Score : {score.item():.4f}\\n\"\n",
    "        source_id_text = f\"Id : {source_id}\\n\"\n",
    "        main_text = (\n",
    "            f\"\"\"{(\" \".join(df.loc[df[\"source_id\"] == source_id, \"text\"].values))}\"\"\"\n",
    "        )\n",
    "        results_text += (\n",
    "            query_text\n",
    "            + score_text\n",
    "            + source_id_text\n",
    "            + textwrap.fill(main_text, width=100)\n",
    "            + \"\\n\\n\"\n",
    "        )\n",
    "\n",
    "    return results_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : 'search_top_k_vectors' function executed in 0.22541 seconds.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Perform a semantic search using the indicated query\n",
    "result = retrieve_relevant_sources(\"CI/CD Practices\", df_embeddings, 5)\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate with a local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will write two helper functions to get information about :\n",
    "* Information about the GPU memory for use with CUDA ;\n",
    "* The number of parameters and the memory requirement of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_info() -> dict:\n",
    "    \"\"\"Retrieves information about the GPU memory if CUDA is available.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - total_gpu_memory_gb (float): Total GPU memory in gigabytes.\n",
    "            - reserved_gpu_memory_gb (float): Reserved GPU memory in gigabytes.\n",
    "            - allocated_gpu_memory_gb (float): Allocated GPU memory in gigabytes.\n",
    "            - free_gpu_memory_gb (float): Free GPU memory in gigabytes.\n",
    "    If CUDA is not available, prints a message and returns an empty dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        total_gpu_memory = torch.cuda.mem_get_info()[1]\n",
    "        reserved_gpu_memory = torch.cuda.memory_reserved(0)\n",
    "        allocated_gpu_memory = torch.cuda.memory_allocated(0)\n",
    "        free_gpu_memory = reserved_gpu_memory - allocated_gpu_memory\n",
    "\n",
    "        bytes_to_gb = 1024**3\n",
    "        total_gpu_memory_gb = total_gpu_memory / bytes_to_gb\n",
    "        reserved_gpu_memory_gb = reserved_gpu_memory / bytes_to_gb\n",
    "        allocated_gpu_memory_gb = allocated_gpu_memory / bytes_to_gb\n",
    "        free_gpu_memory_gb = free_gpu_memory / bytes_to_gb\n",
    "\n",
    "        result = {\n",
    "            \"total_gpu_memory_gb\": total_gpu_memory_gb,\n",
    "            \"reserved_gpu_memory_gb\": reserved_gpu_memory_gb,\n",
    "            \"allocated_gpu_memory_gb\": allocated_gpu_memory_gb,\n",
    "            \"free_gpu_memory_gb\": free_gpu_memory_gb,\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_nb_params_and_mem_size(model: torch.nn.Module) -> dict:\n",
    "    \"\"\"Calculate the number of parameters and memory size of a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The PyTorch model to analyze.\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"model_nb_params\" (int): The total number of parameters in the model.\n",
    "            - \"model_memory_requirements_gb\" (float): The total memory requirements of the model in gigabytes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the number of parameters and memory size of the model\n",
    "    nb_parameters = sum([parameter.numel() for parameter in model.parameters()])\n",
    "\n",
    "    # Calculate the memory requirements of the model\n",
    "    mem_parameters = sum(\n",
    "        [\n",
    "            parameter.nelement() * parameter.element_size()\n",
    "            for parameter in model.parameters()\n",
    "        ]\n",
    "    )\n",
    "    mem_buffers = sum(\n",
    "        [buffer.nelement() * buffer.element_size() for buffer in model.buffers()]\n",
    "    )\n",
    "    mem_total = mem_parameters + mem_buffers\n",
    "\n",
    "    result = {\n",
    "        \"model_nb_params\": nb_parameters,\n",
    "        \"model_memory_requirements_gb\": mem_total / 1024**3,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the memory information about our GPU before we load anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_gpu_memory_gb': 3.999755859375,\n",
       " 'reserved_gpu_memory_gb': 1.341796875,\n",
       " 'allocated_gpu_memory_gb': 1.2346582412719727,\n",
       " 'free_gpu_memory_gb': 0.10713863372802734}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load secret token from .env file\n",
    "env_path = Path().cwd() / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "secret_token = os.getenv(\"HUGGING_FACE_ACCESS_TOKEN\")\n",
    "\n",
    "if secret_token is None:\n",
    "    raise ValueError(\"Secret token not found in '.env' file !\")\n",
    "\n",
    "# Login to Hugging Face\n",
    "login(token=secret_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model to use 4-bit quantization for faster performance and lower memory usage\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    if is_flash_attn_2_available()\n",
    "    else torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Using attention implementation : 'sdpa'.\n"
     ]
    }
   ],
   "source": [
    "# Check if flash attention 2 is available and if the GPU supports Flash Attention 2\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "print(f\"INFO : Using attention implementation : '{attn_implementation}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Model set to 'google/gemma-2-2b-it'.\n"
     ]
    }
   ],
   "source": [
    "# Set the model id\n",
    "# model_id = \"google/gemma-7b-it\"\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(f\"INFO : Model set to '{model_id}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c0ee62644840939db84589d44e7f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with the  original configuration and update the hidden activation function\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.hidden_activation = \"gelu_pytorch_tanh\"\n",
    "\n",
    "# Step 3: Load the model with the updated configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_id,\n",
    "    config=config,\n",
    "    quantization_config=nf4_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_nb_params': 1602203904,\n",
       " 'model_memory_requirements_gb': 2.0417227745056152}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_nb_params_and_mem_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_gpu_memory_gb': 3.999755859375,\n",
       " 'reserved_gpu_memory_gb': 5.322265625,\n",
       " 'allocated_gpu_memory_gb': 3.9654006958007812,\n",
       " 'free_gpu_memory_gb': 1.3568649291992188}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_memory_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate text with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See the [Text Generate documentation](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What is expected in the  GCP Data Engineer certification ? Answer in one short sentence.\n",
      "\n",
      "Formatted prompt:\n",
      "<bos><start_of_turn>user\n",
      "What is expected in the  GCP Data Engineer certification ? Answer in one short sentence.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is expected in the  GCP Data Engineer certification ? Answer in one short sentence.\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "chat = [{\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=chat,\n",
    "    tokenize=False,  # keep as raw text (not tokenized)\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(f\"\\nFormatted prompt:\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded model output :\n",
      "<bos><bos><start_of_turn>user\n",
      "What is expected in the  GCP Data Engineer certification ? Answer in one short sentence.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "The GCP Data Engineer certification tests your ability to design, build, and manage data pipelines and infrastructure on Google Cloud Platform. \n",
      "<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input prompt and move it to the GPU\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate model output\n",
    "generated_output = model.generate(\n",
    "    **tokenized_prompt,\n",
    "    max_new_tokens=64\n",
    ")\n",
    "\n",
    "# Decode the model output\n",
    "decoded_output = tokenizer.decode(generated_output[0])\n",
    "\n",
    "# Print the model output\n",
    "print(f\"Decoded model output :\\n{decoded_output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting prompt with context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
